---
title: ARIMA Model
cover: '/cover/ARIMA-Ex.png'
date: 2025-10-12
tags: ['Statistics','TimeSeriesAnalysis']
---

ARIMA (Autoregressive Integrated Moving Average) is a classic statistical model that combines autoregression, differencing (to achieve stationarity), and moving average, and is used to forecast stationary or non‑stationary time series.

[comment]:summary

## Basic Concepts

### Stationarity

If we regard a time series $\{y_t, y_{t-1}, ..., y_0\}$ as observations of a sequence of random variables, where each time $t$ corresponds to a random variable $y_t$:

- Mean $\mu_t = E(y_t)$
- Variance $\sigma_t^2 = E(y_t - \mu_t)^2$
- Autocovariance $\gamma(t,k) = Cov(y_t, y_k) = E[(y_t - \mu_t)(y_k - \mu_k)]$
- Autocorrelation $\rho(t,k) = \frac{\gamma(t,k)}{\sqrt{\sigma_t^2 \times \sigma_k^2}} = \frac{\gamma(t,k)}{\sigma_t \times \sigma_k}$

For forecasting models to be valid, the probability distribution of future data must be consistent with that of historical samples.

In time series analysis, this property is called stationarity. Stationarity is usually divided into two types:

- Strong stationarity
    
    All statistical properties of the sequence do not change over time $t$.
    
    The random variables at any time all come from exactly the same probability distribution.
    
- Weak stationarity
    
    It is sufficient that the low‑order moments of the sequence are constant (independent of time):
    
    - The mean and variance do not change over time $t$:
        - $E(y_t) = E(y_{t-j}) = \mu$
        - $Var(y_t) = Var(y_{t-j}) = \sigma^2$
    - The autocovariance depends only on the time lag $s$, not on the starting point $t$:
        - $Cov(y_t, y_{t-s}) = Cov(y_{t-j}, y_{t-j-s}) = \gamma_s$

As long as a time series satisfies weak stationarity, we can estimate its statistics from historical observations and use them for forecasting:

- Mean estimate $\hat{\mu} = \bar{y} = \frac{1}{T}\sum_{t=1}^{T} y_t$
- Variance estimate $\hat{\sigma}^2 = \frac{1}{T-1}\sum_{t=1}^{T-1} (y_t - \bar{y})^2$
- Autocovariance estimate $\hat{\gamma_s} = \hat{\gamma_{-s}} = \frac{1}{T-s}\sum_{t=1}^{T-s} (y_t - \bar{y})(y_{t+s} - \bar{y})$
- Autocorrelation estimate $\hat{\rho_s} = \hat{\rho_{-s}} = \frac{\hat{\gamma_s}}{\hat{\gamma_0}}$

A special case of a stationary series is white noise (WN) process $\{\varepsilon_t\} \sim \text{WN}(0, \sigma_{\varepsilon}^2)$, whose statistics satisfy:

- Zero mean: $E(\varepsilon_t) = 0$
- Homoscedasticity: $Var(\varepsilon_t) = \sigma_{\varepsilon}^2$
- No autocorrelation: $Cov(\varepsilon_t, \varepsilon_{t-s}) = 0$

In particular, if $\{\varepsilon_t\}$ follows a normal distribution, it is called a Gaussian white noise process.

Although a white noise process is stationary, it is highly random (no correlation between random variables at different times), so it has no value for modeling.

### Random Walk

Random walk is a typical non‑stationary process, widely used in finance and economics. It is often used to describe market behavior:

The asset price at the next time $y_t$ depends only on the previous price $y_{t-1}$, and the price change $\varepsilon_t = y_t - y_{t-1}$ is determined by uncertain factors in the market.

The non‑stationary series generated by a random walk contain two main types of trend components:

- Stochastic trend: sudden and unpredictable changes in direction
- Deterministic trend: a clear long‑term upward or downward trend

Consider the following random walk models:

- Zero‑mean random walk (RW)
    
$$
    y_t = y_{t-1} + \varepsilon_t = (y_{t-2} + \varepsilon_{t-1}) + \varepsilon_t = ... = y_0 + \sum_{i=1}^t \varepsilon_i
$$
    
    - Stochastic trend: the effect of the initial value $y_0$ and the past shocks $\varepsilon_i$ on $y_t$ never decays.
    - Deterministic trend: none.

- Random walk with drift (RWD)
    
$$
    y_t = c + y_{t-1} + \varepsilon_t 
        = c + (c + y_{t-2} + \varepsilon_{t-1}) + \varepsilon_t
        = ...
        = y_0 + c \times t + \sum_{i=1}^t \varepsilon_i
$$

    - Stochastic trend: same as above.
    - Deterministic trend: as time increases, there is a linear drift at rate $c$.

- Random walk with drift and deterministic trend (RWD+DT)
    
$$
    y_t = c_1 + c_2 t + y_{t-1} + \varepsilon_t = y_0 + c_1 t + c_2 \frac{t(t+1)}{2} + \sum_{i=1}^t \varepsilon_i = y_0 + (c_1 + \frac{c_2}{2})t + \frac{c_2}{2} t^2 + \sum_{i=1}^t \varepsilon_i
$$
    
    - Stochastic trend: same as above.
    - Deterministic trend: linear trend + quadratic trend.

If a time series has a stochastic trend, its future observations diverge and cannot converge to a bounded range.

Therefore, before modeling, we need to test the time series to determine whether a stochastic trend is present. Such tests are called unit root tests.

Similarly, deterministic trends also cause divergence of observations, so we must remove both types of trends before modeling to ensure stationarity.

**Differencing**

Stochastic trends and linear deterministic trends can be removed via differencing to obtain a stationary white noise series:

- RW: $y_t - y_{t-1} = \varepsilon_t$
- RWD: $y_t - y_{t-1} = c + \varepsilon_t$

However, non‑linear deterministic trends cannot be removed via simple differencing:

- RWD+DT: $y_t - y_{t-1} = c_1 + c_2 t + \varepsilon_t$

**Regression Approach**

For non‑linear deterministic trends, we can use linear regression to extract a stationary residual term and then build a model on the residuals.

Using RWD+DT as an example:

- Regress $y_t$ on time $t$ to obtain $y_t = a + b t + e_t$.
- Compute residuals $e_t = y_t - a - b t$ to remove the trend.
- Model the stationary residual series $\{e_t, e_{t-1}, ..., e_1\}$.

By applying data transformations and adding non‑linear terms, the regression approach can in principle handle arbitrary non‑linear trends.

We can determine the order of the regression model via t‑tests or F‑tests of the regression coefficients.

### Differencing Transformations

To transform a non‑stationary time series into a stationary one, we usually perform two operations:

- Stabilize the variance: transform the data via log or Box‑Cox.
- Stabilize the mean: remove trend and seasonality via differencing.

Differencing removes stochastic trends and linear deterministic trends and prevents divergence of the series, thereby stabilizing the mean:

- First‑order difference $y'_t = y_t - y_{t-1}$
- Second‑order difference $y''_t = y'_t - y'_{t-1} = y_t - 2y_{t-1} + y_{t-2}$

In addition to trend, differencing can also remove seasonal components (where $m$ is the seasonal length):

- Seasonal difference $y'_t = y_t - y_{t-m}$
- Seasonal + first difference $y^*_t = y'_t - y'_{t-1} = y_t - y_{t-1} - y_{t-m} + y_{t-m-1}$

For strongly seasonal time series, it is usually recommended to perform seasonal differencing first.

If the differenced series is already stationary, there is no need for further differencing.

Over‑differencing introduces unnecessary noise. Take RWD as an example:

- First‑order difference $y'_t = y_t - y_{t-1} = c + \varepsilon_t$
- Second‑order difference $y''_t = y'_t - y'_{t-1} = \varepsilon_t - \varepsilon_{t-1}$

We see that the variance increases significantly after second‑order differencing:

- First‑order: $Var(y'_t) = Var(c) + Var(\varepsilon_t) = \sigma_{\varepsilon}^2$
- Second‑order: $Var(y''_t) = Var(\varepsilon_t) + Var(\varepsilon_{t-1}) = 2\sigma_{\varepsilon}^2$

Moreover, each differencing step reduces one usable data point. Too many differences hurt sample quality.

### Backshift Operator

The backshift notation represents shifting an observation back by one time period:

- First backshift $B y_t = y_{t-1}$
- Second backshift $B^2 y_t = y_{t-2}$
- Seasonal backshift $B^m y_t = y_{t-m}$

Using the backshift operator, differencing can be written as:

- First‑order difference $y'_t = y_t - y_{t-1} = (1 - B)y_t$
- Second‑order difference $y''_t = y'_t - y'_{t-1} = y_t - 2y_{t-1} + y_{t-2} = (1 - B)^2 y_t$
- $d$‑th‑order difference $y^d_t = (1 - B)^d y_t$
- Seasonal difference $y'_t = y_t - y_{t-m} = (1 - B^m)y_t$
- Seasonal + first difference $y^*_t = y'_t - y'_{t-1} = (1 - B)(1 - B^m)y_t$

Backshift operators can be multiplied, which makes it easy to see the actual effect of combined differences:

$$
(1 - B)(1 - B^m)y_t = (1 - B - B^m + B^{m+1})y_t
$$

This property can also be used to determine whether a model contains redundant terms.

Suppose we have an ARMA model $y_t = 0.5 y_{t-1} + 0.24 y_{t-2} + e_t + 0.6 e_{t-1} + 0.09 e_{t-2}$.

Using the backshift notation, this becomes $(1 + 0.3B)(1 - 0.8B) y_t = (1 + 0.3B)(1 + 0.3B) e_t$.

By canceling common factors, it can be simplified to a lower‑order model $(1 - 0.8B) y_t = (1 + 0.3B) e_t$.

## AR Model

An autoregressive model (AR) is a multivariate regression model based on lagged values.

The number of lagged terms is called the order of the AR model. A $p$‑th‑order AR model can be written as:

$$
\text{AR}(p):\quad y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \varepsilon_t
$$

- $\phi_i$ are autoregressive coefficients.
- $\varepsilon_t$ is white noise.

The simplest case is $\text{AR}(1): y_t = c + \phi_1 y_{t-1} + \varepsilon_t$.

- $\phi_1 = 0, c = 0 \ \to\ y_t = \varepsilon_t$
    
    The $\text{AR}(1)$ process reduces to WN.
    
- $\phi_1 = 1, c = 0 \ \to\ y_t = y_{t-1} + \varepsilon_t$
    
    The $\text{AR}(1)$ process reduces to RW.
    
- $\phi_1 = 1, c \ne 0 \ \to\ y_t = c + y_{t-1} + \varepsilon_t$
    
    The $\text{AR}(1)$ process reduces to RWD.
    
- $|\phi_1| < 1$
    
    The $\text{AR}(1)$ process is stationary.
    
- $\phi_1 < 0$
    
    The $\text{AR}(1)$ process oscillates between positive and negative values.
    
From these examples we see that only when the autoregressive coefficients satisfy certain conditions is the $\text{AR}(p)$ process stationary.

### Stationarity

For an $\text{AR}(p)$ process we can construct a $p$‑dimensional coefficient matrix $A$:

- **First row**: contains the model parameters $\phi_i$, representing the influence of each lagged value on the current value.
- **Subsequent rows**: form the lower part of an identity matrix, describing how the lagged variables $Y_{t-1}, ..., Y_{t-p}$ affect the state.

$$
A = \begin{bmatrix}
\phi_1 & \phi_2 & \cdots & \phi_p\\
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix}
$$

Let $\lambda$ be an eigenvalue and construct the characteristic equation $\det(A - \lambda I) = 0$, which leads to

$$
\lambda^p - \phi_1 \lambda^{p-1} - \phi_2 \lambda^{p-2} - ... - \phi_p = 0.
$$

Let $z = 1/\lambda$ to rewrite it as

$$
1 - \phi_1 z - \phi_2 z^2 - ... - \phi_p z^p = 0.
$$

Solving this equation yields the characteristic roots $\lambda_1, \lambda_2, ..., \lambda_p$. Their magnitudes describe how strongly past observations affect the current observation:

- $|\lambda| < 1$: impact decays; the series reverts to the mean.
- $|\lambda| = 1$: impact persists; may cause a trend.
- $|\lambda| > 1$: impact grows; the series diverges.

Since characteristic roots are often complex, we usually plot them on the complex plane using the unit circle (radius 1, centered at the origin):

- $|\lambda| < 1$: the root is inside the unit circle.
- $|\lambda| \ge 1$: the root lies on or outside the unit circle.

Stationarity of an $\text{AR}(p)$ model can be expressed in terms of unit roots:

- Stationary model: no unit roots, $\forall |\lambda_i| < 1$.
- Non‑stationary model: at least one unit root, $\exists |\lambda_i| = 1$.

For $\text{AR}(1)$ the characteristic equation is

$$
\lambda^1 - \phi_1 \lambda^0 = \lambda - \phi_1 = 0.
$$

The root is $\lambda = \phi_1$. As long as $|\phi_1| < 1$, the model is stationary.

For $\text{AR}(2)$, the characteristic equation is

$$
\lambda^2 - \phi_1 \lambda^1 - \phi_2 \lambda^0 = \lambda^2 - \phi_1 \lambda - \phi_2 = 0.
$$

According to the quadratic formula, when the discriminant $\Delta = b^2 - 4ac = (-\phi_1)^2 + 4 \phi_2 \ge 0$, there are two real roots

$$
\lambda = \frac{\phi_1 \pm \sqrt{\phi_1^2 + 4 \phi_2}}{2}.
$$

If $|\phi_2| < 1$ and $\phi_2 \pm \phi_1 < 1$ hold simultaneously, the model is stationary.

### Moments

For a stationary $\text{AR}(p)$ process, the mean is

$$
E(y_t) = E(c) + \phi_1 E(y_{t-1}) + \phi_2 E(y_{t-2}) + \cdots + E(\varepsilon_t).
$$

For a stationary process, $E(y_t) = E(y_{t-1}) = \cdots = \mu$.

Substituting gives

$$
\mu = E(c) + \phi_1 \mu + \phi_2 \mu + \cdots + E(\varepsilon_t),
$$

which yields

$$
\mu = \frac{c}{1 - \phi_1 - \phi_2 - \cdots - \phi_p}.
$$

For the variance of a stationary $\text{AR}(p)$ process,

$$
Var(y_t) = Var(c) + \phi_1 Var(y_{t-1}) + \phi_2 Var(y_{t-2}) + \cdots + Var(\varepsilon_t).
$$

For stationarity we have $Var(y_t) = Var(y_{t-1}) = \cdots = \sigma_y^2$. Solving gives

$$
\sigma_y^2 = \frac{\sigma_{\varepsilon}^2}{1 - \phi_1 - \phi_2 - \cdots - \phi_p}.
$$

The autocovariance of a stationary $\text{AR}(p)$ process can be computed recursively:

- $Cov(y_t, y_{t-1}) = \phi_1 \sigma_y^2$
- $Cov(y_t, y_{t-s}) = \sum_{i=1}^p \phi_i Cov(y_{t-i}, y_{t-s})$

### Forecast Error

Let $\hat{y}_{t+l|t}$ denote the $l$‑step‑ahead forecast based on all observations up to time $t$.

Given information set $I_t$, the mean squared forecast error is

$$
MSE(\hat{y}_{t+l}|I_t) = E[(y_{t+l} - \hat{y}_{t+l})^2 | I_t].
$$

The conditional expectation $E[y_{t+l} | I_t]$ is the $l$‑step forecast that minimizes MSE, so

$$
e_t(l) = y_{t+l} - \hat{y}_{t+l|t} = y_{t+l} - E[y_{t+l} | I_t]
$$

is the $l$‑step forecast error.

**One‑step‑ahead forecast for $\text{AR}(p)$**

Model:

$$
y_{t+1} = c + \phi_1 y_t + \phi_2 y_{t-1} + \cdots + \phi_p y_{t-p+1} + \varepsilon_{t+1}.
$$

Conditional expectation:

$$
\hat{y}_{t+1|t} = E(y_{t+1} | I_t) = E(c + \phi_1 y_t + \phi_2 y_{t-1} + \cdots + \phi_p y_{t-p+1} + \varepsilon_{t+1} | I_t).
$$

Given $I_t = \{y_t, y_{t-1}, ..., y_{t-p}\}$, $y_i$ $(i = t, t-1, ..., t-p+1)$ are constants, so

$$
\hat{y}_{t+1|t} = c + \phi_1 y_t + \phi_2 y_{t-1} + \cdots + \phi_p y_{t-p+1}.
$$

- One‑step forecast error: $e_t(1) = y_{t+1} - \hat{y}_{t+1|t} = \varepsilon_{t+1}$.
- Variance: $Var(e_t(1)) = Var(\varepsilon_{t+1}) = \sigma_\varepsilon^2$.
- If $\varepsilon_t$ is normal, the 95% one‑step prediction interval for $y_{t+1}$ is $\hat{y}_{t+1|t} \pm 1.96 \sigma_\varepsilon$.

**Two‑step‑ahead forecast for $\text{AR}(p)$**

Model:

$$
y_{t+2} = c + \phi_1 y_{t+1} + \phi_2 y_t + \cdots + \phi_p y_{t-p+2} + \varepsilon_{t+2}.
$$

Conditional expectation:

$$
\hat{y}_{t+2|t} = E(y_{t+2} | I_t) = E(c + \phi_1 y_{t+1} + \phi_2 y_t + \cdots + \phi_p y_{t-p+2} + \varepsilon_{t+2} | I_t).
$$

Here $y_{t+1}$ is unknown, so we replace it with its forecast $\hat{y}_{t+1|t}$:

$$
\hat{y}_{t+2|t} = c + \phi_1 \hat{y}_{t+1|t} + \phi_2 y_t + \cdots + \phi_p y_{t-p+2}.
$$

- Two‑step forecast error: $e_t(2) = y_{t+2} - \hat{y}_{t+2|t} = \phi_1 \varepsilon_{t+1} + \varepsilon_{t+2}$.
- Variance: $Var(e_t(2)) = Var(\phi_1 \varepsilon_{t+1} + \varepsilon_{t+2}) = (\phi_1^2 + 1) \sigma_\varepsilon^2$.
- If $\varepsilon_t$ is normal, the 95% two‑step prediction interval for $y_{t+2}$ is $\hat{y}_{t+2|t} \pm 1.96 \sqrt{(\phi_1^2 + 1) \sigma_\varepsilon^2}$.

**$l$‑step‑ahead forecast for $\text{AR}(p)$**

Model:

$$
y_{t+l} = c + \phi_1 y_{t+l-1} + \phi_2 y_{t+l-2} + \cdots + \phi_p y_{t-p+l} + \varepsilon_{t+l}.
$$

Conditional expectation:

$$
\hat{y}_{t+l|t} = E(y_{t+l} | I_t).
$$

Given $I_t = \{y_t, y_{t-1}, ..., y_{t-p}\}$, we have

$$
\hat{y}_{t+l|t} = c + \sum_{i=1}^p \phi_i \hat{y}_{t+l-i|t}.
$$

As $l \to \infty$, $\hat{y}_{t+l|t}$ converges to $E(y_t)$, i.e. long‑term forecasts converge to the unconditional mean $\mu$.

- $l$‑step forecast error:

$$
\begin{aligned}
e_t(l) &= y_{t+l} - \hat{y}_{t+l|t} \\
&= \sum_{i=1}^p \phi_i y_{t+l-i} - \sum_{i=1}^p \phi_i \hat{y}_{t+l-i|t} + \varepsilon_{t+l} \\
&= \phi_1 (y_{t+l-1} - \hat{y}_{t+l-1|t}) + \phi_2 (y_{t+l-2} - \hat{y}_{t+l-2|t}) + \cdots + \varepsilon_{t+l} \\
&= \sum_{i=1}^p \phi_i \varepsilon_{t+l-i} + \varepsilon_{t+l}.
\end{aligned}
$$

- Variance of $l$‑step forecast error:

$$
\begin{aligned}
Var(e_t(l)) &= Var\Big(\sum_{i=1}^p \phi_i \varepsilon_{t+l-i} + \varepsilon_{t+l}\Big) \\
&= \phi_1^2 Var(\varepsilon_{t+l-1}) + \phi_2^2 Var(\varepsilon_{t+l-2}) + \cdots + Var(\varepsilon_{t+l}) \\
&= (\phi_1^2 + \phi_2^2 + \cdots + \phi_p^2 + 1) \sigma_\varepsilon^2.
\end{aligned}
$$

### Autocorrelation Function

The autocorrelation function (ACF) captures **indirect** correlation: the cumulative impact of all random variables $y_{t-1}, ..., y_{t-s}$ within lag $s$ on $y_t$.

The partial autocorrelation function (PACF) captures **direct** correlation: the pure correlation between $y_t$ and $y_{t-s}$ after removing the effects of intermediate lags.

For an $\text{AR}(p)$ model, the autocovariance is

$$
\begin{aligned}
\gamma_s &= Cov(y_t, y_{t-s}) \\
&= E[(y_t - c)(y_{t-s} - c)] \\
&= E[(\phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \varepsilon_t)(y_{t-s} - c)] \\
&= \phi_1 E(y_{t-1} y_{t-s}) + \phi_2 E(y_{t-2} y_{t-s}) + \cdots + \phi_p E(y_{t-p} y_{t-s}) + E(\varepsilon_t y_{t-s}) \\
&= \phi_1 \gamma_{s-1} + \phi_2 \gamma_{s-2} + \cdots + \phi_p \gamma_{s-p} + E(\varepsilon_t y_{t-s}).
\end{aligned}
$$

The autocorrelation is

$$
\rho_s = \frac{\gamma_s}{\gamma_0} = \phi_1 \rho_{s-1} + \phi_2 \rho_{s-2} + \cdots + \phi_p \rho_{s-p} + \frac{E(\varepsilon_t y_{t-s})}{\gamma_0},
$$

where $\gamma_0 = Var(y_t)$ is simply the variance of the series.

When $s = 0$,

$$
1 = \rho_0 = \phi_1 \rho_1 + \phi_2 \rho_2 + \cdots + \phi_p \rho_p + \frac{E(\varepsilon_t y_t)}{\gamma_0}.
$$

Autocorrelation is symmetric, so the above holds as written.

Since $\varepsilon_t$ is uncorrelated with $y_{t-1}, y_{t-2}, ...$, but correlated with $y_t$, we have $E(\varepsilon_t y_t) \ne E(\varepsilon_t) E(y_t)$. Expanding,

$$
\begin{aligned}
E(\varepsilon_t y_t)
&= E\big[\varepsilon_t (c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p}) + \varepsilon_t^2\big] \\
&= E\big[\varepsilon_t (c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p})\big] + E(\varepsilon_t^2) \\
&= E(\varepsilon_t) E(c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p}) + E(\varepsilon_t^2) \\
&= 0 \times E(c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p}) + \sigma_\varepsilon^2.
\end{aligned}
$$

Thus,

$$
\rho_0 = \phi_1 \rho_1 + \phi_2 \rho_2 + \cdots + \phi_p \rho_p + \frac{\sigma_\varepsilon^2}{\gamma_0}.
$$

When $s > 0$, $y_{t-s}$ cannot be $y_t$, so $\varepsilon_t$ is uncorrelated with $y_{t-s}$ and

$$
E(\varepsilon_t y_{t-s}) = E(\varepsilon_t) E(y_{t-s}) = 0 \times E(y_{t-s}) = 0.
$$

Therefore,

$$
\rho_s = \phi_1 \rho_{s-1} + \phi_2 \rho_{s-2} + \cdots + \phi_p \rho_{s-p}, \quad s > 0.
$$

Now introduce coefficients $\alpha_i$ to represent the **direct** relationship between $y_t$ and $y_{t-i}$ (lag $i$).

From $\rho_s = \phi_1 \rho_{s-1} + \cdots + \phi_p \rho_{s-p}$, we obtain the linear system:

$$
\begin{cases}
\rho_1 = \alpha_1 \rho_0 + \cdots + \alpha_p \rho_{p-1} \\
\rho_2 = \alpha_1 \rho_1 + \cdots + \alpha_p \rho_{p-2} \\
\vdots \\
\rho_s = \alpha_1 \rho_{s-1} + \cdots + \alpha_p \rho_{s-p}
\end{cases}
\ \to\
\begin{bmatrix}
\rho_0 & \rho_1 & \cdots & \rho_{p-1} \\
\rho_1 & \rho_0 & \cdots & \rho_{p-2} \\
\vdots & \vdots & \ddots & \vdots \\
\rho_{s-1} & \rho_{s-2} & \cdots & \rho_{s-p}
\end{bmatrix}
\begin{bmatrix}
\alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_p
\end{bmatrix}
=
\begin{bmatrix}
\rho_1 \\ \rho_2 \\ \vdots \\ \rho_s
\end{bmatrix}
\ \to\ R \boldsymbol{\alpha} = \boldsymbol{\rho}.
$$

Solving $\boldsymbol{\alpha} = R^{-1} \boldsymbol{\rho}$ gives the partial autocorrelations $\alpha_1, ..., \alpha_p$.

For $s > p$, the autoregressive coefficient of $y_{t-s}$ is zero, so there is no direct correlation between $y_t$ and $y_{t-s}$.

The PACF of an $\text{AR}(p)$ model is

$$
\alpha_s =
\begin{cases}
1, & s = 0, \\
\alpha_s, & 1 \le s \le p, \\
0, & s > p.
\end{cases}
$$

### Parameter Estimation

#### Yule–Walker

For a zero‑mean $\text{AR}(p)$ process, move terms to the left and multiply both sides by $y_{t-h}$ $(h \ge 0)$:

$$
y_{t-h} (y_t - \phi_1 y_{t-1} - \phi_2 y_{t-2} - \cdots - \phi_p y_{t-p}) = y_{t-h} \varepsilon_t.
$$

Take expectation and express in terms of autocovariances $\gamma_i = E(y_t y_{t-i})$:

$$
\gamma_h - \sum_{i=1}^p \phi_i \gamma_{h-i} = E(y_{t-h} \varepsilon_t) =
\begin{cases}
\sigma_\varepsilon^2, & h = 0, \\
0, & h \ge 1.
\end{cases}
$$

For $h = 1, ..., p$ we have

$$
\gamma_h - \sum_{i=1}^p \phi_i \gamma_{h-i} = 0.
$$

This gives the Yule–Walker equations:

$$
\begin{bmatrix}
\gamma_0 & \gamma_1 & \cdots & \gamma_{p-1} \\
\gamma_1 & \gamma_0 & \cdots & \gamma_{p-2} \\
\vdots & \vdots & \ddots & \vdots \\
\gamma_{p-1} & \gamma_{p-2} & \cdots & \gamma_0
\end{bmatrix}
\begin{bmatrix}
\phi_1 \\ \phi_2 \\ \vdots \\ \phi_p
\end{bmatrix}
=
\begin{bmatrix}
\gamma_1 \\ \gamma_2 \\ \vdots \\ \gamma_p
\end{bmatrix}
\ \to\ \boldsymbol{\Gamma} \boldsymbol{\phi} = \boldsymbol{\gamma}.
$$

Plug in the sample autocovariances $\hat{\gamma}_h = \big(\sum_{t=1}^{n-|h|} y_t y_{t+|h|}\big)/n$ to obtain

$$
\boldsymbol{\hat{\phi}} = \boldsymbol{\Gamma}^{-1} \boldsymbol{\hat{\gamma}}.
$$

From $\gamma_0 - \sum_{i=1}^p \phi_i \gamma_{0-i} = \sigma_\varepsilon^2$ we obtain

$$
\hat{\sigma}_\varepsilon^2 = \gamma_0 - \sum_{i=1}^p \phi_i r_i.
$$

#### Burg

If the matrix $\boldsymbol{\Gamma}$ is ill‑conditioned, the Yule–Walker method becomes very sensitive to outliers, making the estimates $\boldsymbol{\hat{\phi}}$ unstable.

To improve stability, the Burg algorithm is often used to estimate $\text{AR}(p)$ parameters, especially for short series.

Burg’s method estimates AR parameters by minimizing both forward and backward prediction errors.

Consider two $\text{AR}(k)$ models:

- Forward model (predicting current from past):
    
    $$
    y_t^+ = \phi_1 y_{t-1} + \cdots + \phi_k y_{t-k} + \varepsilon_t^+, \quad t \in [k, n].
    $$
    
    The errors are called **forward errors** and reflect predictive performance on future data.
- Backward model (predicting current from future):
    
    $$
    y_t^- = \phi_1 y_{t+1} + \cdots + \phi_k y_{t+k} + \varepsilon_t^-, \quad t \in [0, n-k].
    $$
    
    The errors are called **backward errors** and reflect fit on past data.

Burg minimizes the sum of forward and backward squared errors:

- Forward error:

    $$
    F_k = \sum_{t=k}^n (y_t - y_t^+)^2 = \sum_{t=k}^n \Big(y_t - \sum_{i=1}^k \phi_i y_{t-i}\Big)^2.
    $$

- Backward error:

    $$
    B_k = \sum_{t=0}^{n-k} (y_t - y_t^-)^2 = \sum_{t=0}^{n-k} \Big(y_t - \sum_{i=1}^k \phi_i y_{t+i}\Big)^2.
    $$

Let $\phi_0 = 1$. Define

- $F_k = \sum_{t=k}^n (f_k(t))^2$, where $f_k(t) = \sum_{i=0}^k \phi_i y_{t-i}$.
- $B_k = \sum_{t=0}^{n-k} (b_k(t))^2$, where $b_k(t) = \sum_{i=0}^k \phi_i y_{t+i}$.

Burg uses a recursive update

$$
A_{k+1} = A_k + \mu V_k,
$$

where

- $A_k = [1, \phi_1, ..., \phi_k]$,
- $V_k = [0, \phi_k, ..., \phi_2, \phi_1, 1]$.

Let $\phi_{k+1} = 1$. The recursion can be written component‑wise as

$$
\phi_i' = \phi_i + \mu \phi_{k+1-i}.
$$

To ensure $A_{k+1}$ is better than $A_k$, we choose $\mu$ to minimize $F_{k+1} + B_{k+1}$.

Expanding $F_{k+1} + B_{k+1} = \sum_{t=k+1}^n f_{k+1}(t)^2 + \sum_{t=0}^{n-k-1} b_{k+1}(t)^2$ yields

- $f_{k+1}(t) = f_k(t) + \mu b_k(t-k-1)$,
- $b_{k+1}(t) = b_k(t) + \mu f_k(t+k+1)$.

Setting $\partial(F_{k+1} + B_{k+1})/\partial \mu = 0$ gives

$$
\mu = \frac{-2 \sum_{t=0}^{n-k-1} f_k(t+k+1) b_k(t)}{\sum_{t=k+1}^n f_k(t)^2 + \sum_{t=0}^{n-k-1} b_k(t)^2}.
$$

The overall Burg algorithm is:

- Choose order $p$.
- Initialize $A_0 = [1]$.
- Initialize $f_0(t) = b_0(t) = y_t$.
- For $k = 0, ..., p-1$:
    - Compute $\mu$ and update $A_{k+1}$.
    - Update $f_{k+1}(t)$ for $t \in [k+1, n]$.
    - Update $b_{k+1}(t)$ for $t \in [0, n-k-1]$.

A detailed derivation can be found in, e.g.,

https://c.mql5.com/3/133/Tutorial_on_Burg_smethod_algorithm_recursion.pdf

## MA Model

A moving average model (MA) is a multivariate regression model based on **past errors**.

The number of error terms is called the order of the MA model. A $q$‑th‑order MA model is

$$
\text{MA}(q):\quad y_t = c + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \cdots + \theta_q \varepsilon_{t-q},
$$

- $\theta_i$ are error weights.
- $\varepsilon_t$ is white noise.

### Moments

For an $\text{MA}(q)$ process:

- Mean: $E(y_t) = c$.
- Variance: $Var(y_t) = (1 + \theta_1^2 + \theta_2^2 + \cdots + \theta_q^2) \sigma_\varepsilon^2$.
- Autocovariance:

$$
Cov(y_t, y_{t-s}) = E[(y_t - c)(y_{t-s} - c)] =
\begin{cases}
(1 + \theta_1^2 + \cdots + \theta_q^2) \sigma_\varepsilon^2, & s = 0, \\
(\theta_s + \theta_1 \theta_{s+1} + \cdots + \theta_{q-s} \theta_q) \sigma_\varepsilon^2, & 1 \le s \le q, \\
0, & s > q.
\end{cases}
$$

A finite‑order $\text{MA}(q)$ model is always stationary, with no constraints on $\theta$ needed.

### Invertibility

A stationary $\text{AR}(p)$ process can be rewritten as an $\text{MA}(\infty)$ process. For example,

$$
\begin{aligned}
y_t &= \phi_1 y_{t-1} + \varepsilon_t \\
&= \phi_1(\phi_1 y_{t-2} + \varepsilon_{t-1}) + \varepsilon_t \\
&= \phi_1^2 y_{t-2} + \phi_1 \varepsilon_{t-1} + \varepsilon_t \\
&= \phi_1^3 y_{t-3} + \phi_1^2 \varepsilon_{t-2} + \phi_1 \varepsilon_{t-1} + \varepsilon_t \\
&= \cdots.
\end{aligned}
$$

If $|\phi_1| < 1$, then $\phi_1^{\infty} y_{t-\infty} = 0$.

Conversely, an invertible $\text{MA}(q)$ can be represented as a convergent $\text{AR}(\infty)$:

$$
\begin{aligned}
y_t &= \varepsilon_t + \theta_1 \varepsilon_{t-1} \\
&= \varepsilon_t + \theta_1 (y_{t-1} - \theta_1 \varepsilon_{t-2}) \\
&= \varepsilon_t + \theta_1 y_{t-1} + \theta_1^2 \varepsilon_{t-2} \\
&= \varepsilon_t + \theta_1 y_{t-1} + \theta_1^2 y_{t-2} + \theta_1^3 \varepsilon_{t-3} \\
&= \cdots \\
&= \theta_1 y_{t-1} + \theta_1^2 y_{t-2} + \theta_1^3 y_{t-3} + \cdots + \varepsilon_t.
\end{aligned}
$$

Invertibility of $\text{MA}(q)$ can also be expressed via the characteristic equation

$$
1 - \theta_1 z - \theta_2 z^2 - \cdots - \theta_q z^q = 0.
$$

The condition is similar to the AR case:

- Invertible: no unit roots, $\forall |\lambda_i| < 1$.
- Non‑invertible: at least one unit root, $\exists |\lambda_i| = 1$.

For $\text{MA}(1)$, the characteristic equation is

$$
\lambda^1 - \theta_1 \lambda^0 = \lambda - \theta_1 = 0,
$$

so $|\theta_1| < 1$ ensures invertibility.

For $\text{MA}(2)$,

$$
\lambda^2 - \theta_1 \lambda^1 - \theta_2 \lambda^0 = \lambda^2 - \theta_1 \lambda - \theta_2 = 0,
$$

and invertibility holds if $|\theta_2| < 1$ and $\theta_2 \pm \theta_1 < 1$.

An invertible and a non‑invertible $\text{MA}(q)$ can produce the same first and second moments. For example, consider an invertible $\text{MA}(1)$:

$$
y_t = c + \varepsilon_t + \theta_1 \varepsilon_{t-1}, \quad \varepsilon_t \sim D(0, \sigma_\varepsilon^2).
$$

- Mean: $E(y_t) = c$.
- Variance: $Var(y_t) = \sigma_\varepsilon^2 + \theta_1^2 \sigma_\varepsilon^2$.

And a non‑invertible counterpart:

$$
y_t^* = c + \varepsilon_t^* + \frac{1}{\theta_1} \varepsilon_{t-1}^*, \quad \varepsilon_t^* \sim D(0, \theta_1^2 \sigma_\varepsilon^2).
$$

- Mean: $E(y_t^*) = c$.
- Variance: $Var(y_t^*) = \theta_1^2 \sigma_\varepsilon^2 + \sigma_\varepsilon^2$.

However, the two processes fundamentally differ in how $\varepsilon_t$ is computed:

- Non‑invertible model: $\varepsilon_t$ depends on future values $y_{t+1}, y_{t+2}, ...$.
- Invertible model: $\varepsilon_t$ depends only on current and past values $y_t, y_{t-1}, ...$.

When performing forecasting or maximum likelihood estimation, we need to compute $\varepsilon_t$, and an invertible $\text{MA}(q)$ is much more convenient.

In other words, only invertible $\text{MA}(q)$ models are usable for forecasting.

### Forecast Error

#### $\text{MA}(1)$

Model:

$$
y_{t+1} = c + \varepsilon_{t+1} + \theta_1 \varepsilon_t.
$$

Conditional expectation:

$$
\hat{y}_{t+1|t} = c + E(\varepsilon_{t+1} | I_t) + \theta_1 E(\varepsilon_t | I_t).
$$

Assume we have an invertible model. Given $I_t = \{y_t, y_{t-1}, ..., y_1\}$, $c, \theta_1, y_i$ are constants, and we can compute residuals $\varepsilon_t$ recursively:

- $y_1 = c + \varepsilon_1 \ \to\ \varepsilon_1 = y_1 - c$.
- $y_2 = c + \varepsilon_2 + \theta_1 \varepsilon_1 \ \to\ \varepsilon_2 = y_2 - c - \theta_1 (y_1 - c)$.
- ...
- $y_t = c + \varepsilon_t + \theta_1 \varepsilon_{t-1} \ \to\ \varepsilon_t = y_t - c - \theta_1 \varepsilon_{t-1}$.

Unconditionally, $E(\varepsilon_t) = 0$, but under $I_t$, $\varepsilon_i$ $(i = 1, ..., t)$ are constants, so

$$
E(\varepsilon_{t+i} | I_t) =
\begin{cases}
0, & i > 0, \\
\varepsilon_{t+i}, & i \le 0.
\end{cases}
$$

Hence

$$
\hat{y}_{t+1|t} = c + \theta_1 \varepsilon_t.
$$

- One‑step forecast error: $e_t(1) = y_{t+1} - \hat{y}_{t+1|t} = \varepsilon_{t+1}$.
- Variance: $Var(e_t(1)) = \sigma_\varepsilon^2$.

For the two‑step forecast,

$$
\hat{y}_{t+2|t} = c + \theta_1 E(\varepsilon_{t+1} | I_t) = c.
$$

In general, for $l \ge 2$,

$$
\hat{y}_{t+l|t} = c.
$$

#### $\text{MA}(2)$

Model:

$$
y_{t+1} = c + \varepsilon_{t+1} + \theta_1 \varepsilon_t + \theta_2 \varepsilon_{t-1}.
$$

One‑step conditional expectation:

$$
\hat{y}_{t+1|t} = c + \theta_1 \varepsilon_t + \theta_2 \varepsilon_{t-1}.
$$

- One‑step forecast error: $e_t(1) = \varepsilon_{t+1}$.
- Variance: $Var(e_t(1)) = \sigma_\varepsilon^2$.

Two‑step model:

$$
y_{t+2} = c + \varepsilon_{t+2} + \theta_1 \varepsilon_{t+1} + \theta_2 \varepsilon_t.
$$

Two‑step conditional expectation:

$$
\hat{y}_{t+2|t} = c + \theta_2 \varepsilon_t.
$$

- Two‑step forecast error: $e_t(2) = \varepsilon_{t+2} + \theta_1 \varepsilon_{t+1}$.
- Variance: $Var(e_t(2)) = (1 + \theta_1^2) \sigma_\varepsilon^2$.

For $l \ge 3$, $\hat{y}_{t+l|t} = c$.

#### General $\text{MA}(q)$

Model:

$$
y_{t+l} = c + \varepsilon_{t+l} + \theta_1 \varepsilon_{t+l-1} + \theta_2 \varepsilon_{t+l-2} + \cdots + \theta_q \varepsilon_{t+l-q}.
$$

Given $I_t$, we have

$$
\begin{aligned}
\hat{y}_{t+l|t} &= E(y_{t+l} | I_t) \\
&= c + E(\varepsilon_{t+l} | I_t) + \theta_1 E(\varepsilon_{t+l-1} | I_t) + \cdots + \theta_q E(\varepsilon_{t+l-q} | I_t).
\end{aligned}
$$

Thus

$$
\hat{y}_{t+l|t} =
\begin{cases}
 c + \theta_l \varepsilon_t + \theta_{l+1} \varepsilon_{t-1} + \cdots + \theta_q \varepsilon_{t+l-q}, & l \le q, \\
 c, & l > q.
\end{cases}
$$

Forecast error:

$$
e_t(l) = y_{t+l} - \hat{y}_{t+l|t} = \varepsilon_{t+l} + \theta_1 \varepsilon_{t+l-1} + \cdots + \theta_q \varepsilon_{t+l-q}.
$$

Variance:

$$
Var(e_t(l)) = (1 + \theta_1^2 + \theta_2^2 + \cdots + \theta_{l-1}^2) \sigma_\varepsilon^2.
$$

If $\varepsilon_t$ is normal, the 95% $l$‑step prediction interval for $y_{t+l}$ is

$$
\hat{y}_{t+l|t} \pm 1.96 \sqrt{Var(e_t(l))}.
$$

### Autocorrelation Function

For $\text{MA}(q)$,

$$
\gamma_s = Cov(y_t, y_{t-s}) = E[(y_t - c)(y_{t-s} - c)] =
\begin{cases}
(1 + \theta_1^2 + \cdots + \theta_q^2) \sigma_\varepsilon^2, & s = 0, \\
(\theta_s + \theta_1 \theta_{s+1} + \cdots + \theta_{q-s} \theta_q) \sigma_\varepsilon^2, & 1 \le s \le q, \\
0, & s > q.
\end{cases}
$$

The autocorrelation is

$$
\rho_s =
\begin{cases}
1, & s = 0, \\
\dfrac{\theta_s + \theta_1 \theta_{s+1} + \cdots + \theta_{q-s} \theta_q}{1 + \theta_1^2 + \cdots + \theta_q^2}, & 1 \le s \le q, \\
0, & s > q.
\end{cases}
$$

Since an invertible $\text{MA}(q)$ can be represented as an $\text{AR}(\infty)$, the PACF of $\text{MA}(q)$ is an infinite‑length tail function and is usually not written out explicitly.

### Parameter Estimation

#### Innovations Algorithm

For a zero‑mean $\text{AR}(t)$ process:

- One‑step forecast: $\hat{y}_{t+1} = \sum_{i=0}^t \phi_{t,i} y_{t-i} = \boldsymbol{\phi_t y_t}$.
- One‑step forecast error: $u_{t+1} = y_{t+1} - \hat{y}_{t+1}$.
- Forecast variance: $v_t = Var(u_{t+1}) = E[(y_{t+1} - \hat{y}_{t+1})^2] = \gamma_0 - \boldsymbol{\phi_t \gamma_t}$.

$v_t$ can be viewed as a mean‑squared error (MSE) function. As long as $v_{t+1} < v_t$, the estimate $\boldsymbol{\hat{\phi}_{t+1}}$ is better than $\boldsymbol{\hat{\phi}_t}$.

The one‑step forecast error $u_{t+1}$ is the **innovation**—new information in $y_{t+1}$ not contained in $I_t = \{y_t, ..., y_1\}$. Thus $u_{t+1}$ is uncorrelated with $I_t$, and also $u_t$ is uncorrelated with $u_s$ for $t \ne s$.

Given observations $\{y_t, ..., y_1\}$ and forecasts $\{\hat{y}_t, ..., \hat{y}_1\}$, with initial best estimate $\hat{y}_1 = 0$, we can express $y_{t+1}$ as a linear combination of past innovations:

$$
y_{t+1} = \sum_{i=1}^t \theta_{t,i} (y_{t+1-i} - \hat{y}_{t+1-i}).
$$

This is the **innovations algorithm**, whose coefficients can be updated iteratively:

- Linear prediction coefficients:

    $$
    \theta_{t,t-k} = \frac{\gamma_{t-k} - \sum_{j=0}^{k-1} \theta_{k,k-j} \theta_{t,t-j} v_j}{v_k}.
    $$

- Forecast variance:

    $$
    v_t = \gamma_0 - \sum_{i=0}^{t-1} \theta_{t,t-i}^2 v_{i+1}.
    $$

For clarity, here are the first three iterations (with $v_0 = \gamma_0$):

1. First iteration:
    
    - $\theta_{1,1} = \dfrac{\gamma_1}{v_0}$.
    - $v_1 = \gamma_0 - \theta_{1,1}^2 v_0$.
2. Second iteration:
    
    - $\theta_{2,2} = \dfrac{\gamma_2}{v_0}$.
    - $\theta_{2,1} = \dfrac{\gamma_1 - \theta_{1,1} \theta_{2,2} v_0}{v_1}$.
    - $v_2 = \gamma_0 - \theta_{2,2}^2 v_0 - \theta_{2,1}^2 v_1$.
3. Third iteration:
    
    - $\theta_{3,3} = \dfrac{\gamma_3}{v_0}$.
    - $\theta_{3,2} = \dfrac{\gamma_2 - \theta_{1,1} \theta_{3,3} v_0}{v_1}$.
    - $\theta_{3,1} = \dfrac{\gamma_1 - (\theta_{2,2} \theta_{3,3} v_0 + \theta_{2,1} \theta_{3,2} v_1)}{v_2}$.
    - $v_3 = \gamma_0 - \theta_{3,3}^2 v_0 - \theta_{3,2}^2 v_1 - \theta_{3,1}^2 v_2$.

The innovations algorithm is often used to estimate $\text{MA}(q)$ parameters. Steps:

- Choose order $q$.
- Compute autocovariances $\gamma_0, ..., \gamma_q$.
- For $m = 1, ..., q$:
    - Initialize $v_0 = \gamma_0$, $\theta_{0,0} = 1$.
    - Iteratively compute $\theta_{m,1}, ..., \theta_{m,m}, v_m$.

#### Hannan–Rissanen

For an $\text{AR}(p)$ model, one can estimate parameters by least squares on

$$
\begin{bmatrix}
 y_p & y_{p-1} & \cdots & y_1 \\
 y_{p+1} & y_p & \cdots & y_2 \\
 \vdots & \vdots & \ddots & \vdots \\
 y_{n-1} & y_{n-2} & \cdots & y_{n-p}
\end{bmatrix}
\begin{bmatrix}
 \phi_1 \\ \phi_2 \\ \vdots \\ \phi_p
\end{bmatrix}
=
\begin{bmatrix}
 y_{p+1} \\ y_{p+2} \\ \vdots \\ y_n
\end{bmatrix}
\ \to\ \boldsymbol{Y \phi = y}.
$$

This does not work directly for $\text{MA}(q)$, because $y_t$ is observable but $\varepsilon_t$ is not.

The Hannan–Rissanen algorithm estimates an $\text{ARMA}(p,q)$ process by first approximating the errors with residuals $e_t$ and then regressing on both lags and residuals. Steps:

1. **Estimate past errors**
    
    Fit a high‑order $\text{AR}(m)$ model with $m > \max(p, q)$ using Yule–Walker to obtain $\hat{\phi}_1, ..., \hat{\phi}_m$, and compute residuals
    
    $$
    e_t = y_t - \hat{\phi}_1 y_{t-1} - \cdots - \hat{\phi}_m y_{t-m}.
    $$

2. **Joint regression**
    
    Build the matrix
    
    $$
    \boldsymbol{A} = \begin{bmatrix}
    y_{m+q} & y_{m+q-1} & \cdots & y_{m+q+1-p} & e_{m+q} & e_{m+q-1} & \cdots & e_{m+1} \\
    y_{m+q+1} & y_{m+q} & \cdots & y_{m+q+2-p} & e_{m+q+1} & e_{m+q} & \cdots & e_{m+2} \\
    \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
    y_{n-1} & y_{n-2} & \cdots & y_{n-p} & e_{n-1} & e_{n-2} & \cdots & e_{n-q}
    \end{bmatrix}.
    $$
    
    Define $\boldsymbol{\beta} = (\phi_1, ..., \phi_p, \theta_1, ..., \theta_q)$, solve $\boldsymbol{A \beta = y}$ to get $\boldsymbol{\hat{\beta}}$, and compute ARMA residuals
    
    $$
    \tilde{e}_t =
    \begin{cases}
    0, & t \le \max(p, q), \\
    y_t - \sum_{j=1}^p \hat{\phi}_j y_{t-j} - \sum_{j=1}^q \hat{\theta}_j \tilde{e}_{t-j}, & t > \max(p, q).
    \end{cases}
    $$

3. **Refine parameter estimates**
    
    Using $\tilde{e}_t$, construct
    
    $$
    \boldsymbol{\tilde{A}} = \begin{bmatrix}
    v_{t-1} & v_{t-2} & \cdots & v_{t-p} & w_{t-1} & w_{t-2} & \cdots & w_{t-q} \\
    v_{t-2} & v_{t-3} & \cdots & v_{t-p-1} & w_{t-2} & w_{t-3} & \cdots & w_{t-q-1} \\
    \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
    v_{n-1} & v_{n-2} & \cdots & v_{n-p} & w_{n-1} & w_{n-2} & \cdots & w_{n-q}
    \end{bmatrix},
    $$
    
    where
    
    - $v_t = \sum_{j=1}^p \hat{\phi}_j v_{t-j} + \tilde{e}_t$,
    - $w_t = \sum_{j=1}^q -\hat{\theta}_j w_{t-j} + \tilde{e}_t$.
    
    Solve $\boldsymbol{\tilde{A} \beta^\dagger = \tilde{e}}$ to obtain $\boldsymbol{\hat{\beta}^\dagger}$ and update parameters $\boldsymbol{\tilde{\beta}} = \boldsymbol{\hat{\beta}} + \boldsymbol{\hat{\beta}^\dagger}$.
    
    This third step is a bias correction and is applied only if the step‑2 estimates correspond to a stationary AR part and an invertible MA part.

## Unit Root Tests

Before analyzing or modeling a time series, we need to determine whether it contains trends.

Unlike deterministic linear trends, stochastic trends cannot be reliably judged by eye.

When differencing is required, we must also choose an appropriate differencing order:

- Remove trends without over‑differencing, which would increase model error.

Unit roots provide a good basis for these decisions:

- If an $\text{AR}(p)$ process has unit roots close to 1, the series needs differencing.
- If an $\text{MA}(q)$ process has unit roots close to 1, the series is likely over‑differenced.

A time series without unit roots but with trend is called **trend‑stationary**. Its trend can be removed via regression without differencing, avoiding unnecessary noise.

A time series with unit roots is called a **unit root process**, or **difference‑stationary**.

If we apply OLS directly to a unit root process, the estimates are biased and t‑statistics for coefficients are invalid. We cannot use regression to remove the trend and must rely on differencing.

Unit root tests are statistical tools to detect these roots. An important application is to select the differencing order: repeatedly difference the data until unit roots are no longer significant.

### ADF Test

Letting $\phi_1 = 1$ in an $\text{AR}(1)$ process yields three common non‑stationary forms:

- $y_t = \phi_1 y_{t-1} + \varepsilon_t$ (RW)
- $y_t = c + \phi_1 y_{t-1} + \varepsilon_t$ (RWD)
- $y_t = c_1 + c_2 t + \phi_1 y_{t-1} + \varepsilon_t$ (RWD+DT)

If we used standard t‑tests on $\phi_1$:

- Null: $H_0: \phi_1 = 1$.
- Estimate $\hat{\phi}_1$ by OLS.
- Compute $t = (\hat{\phi}_1 - 1)/SE(\hat{\phi}_1)$.
- Compare with critical values from the t‑distribution.

The problem is that when $\phi_1 = 1$, the errors $\varepsilon_t$ are heteroscedastic, so OLS is biased and the t‑test is invalid.

The Dickey–Fuller (DF) test solves this by reparameterizing. Starting from

$$
y_t = \phi_1 y_{t-1} + \varepsilon_t,
$$

subtract $y_{t-1}$ from both sides:

- $\Delta y_t = \gamma y_{t-1} + \varepsilon_t$.
- $\Delta y_t = c + \gamma y_{t-1} + \varepsilon_t$.
- $\Delta y_t = c_1 + c_2 t + \gamma y_{t-1} + \varepsilon_t$.

where $\Delta y_t = y_t - y_{t-1}$ and $\gamma = \phi_1 - 1$.

The DF test uses

- $H_0: \gamma = 0$ (unit root present).
- $H_1: \gamma < 0$ (no unit root).

Estimate $\hat{\gamma}$ via OLS and compute

$$
\tau = \frac{\hat{\gamma}}{SE(\hat{\gamma})}.
$$

The distribution of $\tau$ is non‑standard. Dickey and Fuller computed its critical values via Monte Carlo simulation. Note that the three forms above (no constant, constant, constant + trend) have different critical value tables.

If the DF statistic is less than the critical value (one‑sided test), we reject $H_0$ and conclude no unit root.

DF assumes an $\text{AR}(1)$ process and requires uncorrelated errors. For higher‑order $\text{AR}(p)$, unmodeled dynamics leak into the error term, causing autocorrelation.

The augmented Dickey–Fuller (ADF) test extends DF by adding lagged differences to absorb serial correlation in the errors.

Starting from an $\text{AR}(p)$ model, we repeatedly add and subtract terms:

- Add and subtract $\phi_p y_{t-p+1}$:

$$
y_t = c + \phi_1 y_{t-1} + \cdots + (\phi_{p-1} + \phi_p) y_{t-p+1} - \phi_p \Delta y_{t-p+1} + \varepsilon_t.
$$

- Add and subtract $(\phi_{p-1} + \phi_p) y_{t-p+2}$:

$$
y_t = c + \phi_1 y_{t-1} + \cdots + (\phi_{p-2} + \phi_{p-1} + \phi_p) y_{t-p+2} - (\phi_{p-1} + \phi_p) \Delta y_{t-p+2} - \phi_p \Delta y_{t-p+1} + \varepsilon_t.
$$

- Add and subtract $(\phi_{p-2} + \phi_{p-1} + \phi_p) y_{t-p+3}$, and so on.

Eventually we obtain

$$
y_t = c + \Big(\sum_{i=1}^p \phi_i\Big) y_{t-1} - \sum_{i=2}^p \Big(\sum_{j=i}^p \phi_j\Big) \Delta y_{t-i+1} + \varepsilon_t.
$$

In differenced form,

$$
\Delta y_t = c + \gamma y_{t-1} + \sum_{i=2}^p \beta_i \Delta y_{t-i+1} + \varepsilon_t,
$$

where $\gamma = -\big(1 - \sum_{i=1}^p \phi_i\big)$ and $\beta_i = \sum_{j=i}^p \phi_j$.

ADF supports the following three forms:

- $\Delta y_t = \gamma y_{t-1} + \sum_{i=2}^p \beta_i \Delta y_{t-i+1} + \varepsilon_t$.
- $\Delta y_t = c + \gamma y_{t-1} + \sum_{i=2}^p \beta_i \Delta y_{t-i+1} + \varepsilon_t$.
- $\Delta y_t = c_1 + c_2 t + \gamma y_{t-1} + \sum_{i=2}^p \beta_i \Delta y_{t-i+1} + \varepsilon_t$.

These three correspond to different critical value tables and must be specified when testing.

A necessary condition for stationarity of $\text{AR}(p)$ is $\sum_{i=1}^p \phi_i < 1$, so ADF uses the same null and alternative as DF:

- $H_0: \gamma = 0$ (unit root).
- $H_1: \gamma < 0$ (no unit root).

When using ADF we must choose a lag order $p$ and a deterministic form. Lag selection can be done by:

- Starting from a large $p$ and gradually reducing it until all lagged differences are insignificant (if all are insignificant even for large $p$, a unit root is likely present).
- Using information criteria.

An unknown $\text{ARIMA}(p, d, q)$ process can be approximated by an $\text{ARIMA}(n, 1, 0)$ process with $n = T^{1/3}$. Thus ADF can handle data generating processes with unknown MA components $\text{MA}(q)$.

However, ADF often has low power, especially for small samples and processes with strong autocorrelation.

### KPSS Test

ADF has a key limitation: the unit root is the null hypothesis, and low power means that non‑unit root processes are often misclassified as unit root.

The KPSS test (Kwiatkowski–Phillips–Schmidt–Shin) addresses this by reversing the roles:

- Null: stationarity.
- Alternative: unit root.

#### LBI Test

Consider the following state‑space representation of a dynamic system:

- Observation equation: $y_t = x_t \beta_t + z_t' \gamma + \varepsilon_t$.
- State equation: $\beta_t = \beta_{t-1} + u_t$.

The observation has two parts:

- $z_t' \gamma$: time‑invariant component (constant regression coefficients $\gamma$).
- $x_t \beta_t$: time‑varying component (coefficients $\beta_t$ change over time).

There are two error terms:

- Measurement error $\varepsilon_t \sim N(0, \sigma_\varepsilon^2)$ with $\sigma_\varepsilon^2 > 0$.
- Dynamic noise $u_t \sim N(0, \sigma_u^2)$ with $\sigma_u^2 \ge 0$.

This is a **varying‑coefficient regression model**:

- $y_t \in \mathbb{R}$ is the dependent variable.
- $x_t, z_t \in \mathbb{R}^n$ are regressors.
- $\beta_t, \gamma \in \mathbb{R}^n$ are coefficients.
- $\sigma_\varepsilon^2$ is the variance of the observation error.
- $\sigma_u^2$ is the variance of the state innovation.

When $\sigma_u^2 > 0$, the time‑varying coefficient $\beta_t$ follows a random walk. If $y_t$ is normal, the LBI (Locally Best Invariant) test can be used to test constancy of $\beta_t$:

- $H_0: \rho = \sigma_\varepsilon^2 / \sigma_u^2 = 0$ (no relation between observation and state errors).
- $H_1: \rho > 0$ (relation exists).

The KPSS test is a special case with $x_t = 1$, $\beta_t = r_t$, $z_t = t$, $\gamma = \xi$:

$$
y_t = \xi t + r_t + \varepsilon_t,
$$

where

- Deterministic trend: $\xi t$.
- Random walk: $r_t = r_{t-1} + u_t$, with $r_0$ initial and $u_t \sim N(0, \sigma_u^2)$.
- Stationary error: $\varepsilon_t \sim N(0, \sigma_\varepsilon^2)$.

The KPSS hypotheses are

- $H_0: \sigma_u^2 = 0$ (stationary).
- $H_1: \sigma_u^2 \ne 0$ (non‑stationary).

KPSS supports two cases:

- $\xi \ne 0$: $H_0$ is **trend stationarity**.
- $\xi = 0$: $H_0$ is **level stationarity**.

When $\varepsilon_t \sim N(0, \sigma_\varepsilon^2)$, KPSS is a standard LM test (LBI is a special case). Steps:

1. Compute residuals $e_i$:
    - If $\xi \ne 0$, regress $y_t$ on $t$ by OLS: $y_t = r_0 + \xi t$.
    - If $\xi = 0$, use $e_i = y_i - \bar{y}$.
2. Compute LM statistic:
    - Cumulative sum of residuals: $S_t = \sum_{i=1}^t e_i$.
    - Residual variance: $\hat{\sigma}_\varepsilon^2 = Var(e_i)$.
    - LM statistic: $LM = \sum_{t=1}^T S_t^2 / \hat{\sigma}_\varepsilon^2$.
3. Compare to critical values (or p‑values) to decide whether to reject $H_0$.

In practice, errors are usually serially correlated, so $\varepsilon_t \sim N(0, \sigma_\varepsilon^2)$ is too strong. KPSS therefore uses a weaker asymptotic framework.

- Define long‑run variance $\sigma^2 = \lim_{T \to \infty} E(S_T^2)/T$.
- Estimate it via a Newey–West‑type estimator

    $$
    s^2(l) = T^{-1} \sum_{t=1}^T e_t^2 + 2T^{-1} \sum_{s=1}^l w(s, l) \sum_{t=s+1}^T e_t e_{t-s},
    $$
    
    where $w(s, l) = 1 - s/(l+1)$ is a Bartlett kernel and $l = o(T^{1/2})$ is the lag truncation.

- Use $s^2(l)$ as $\hat{\sigma}_\varepsilon^2$ in the LM statistic.
- Normalize by $T^{-2}$: $\eta = T^{-2} \sum S_t^2$.

Define

- $\hat{\eta}_\mu = \eta_\mu / s^2(l)$ (level stationarity).
- $\hat{\eta}_\tau = \eta_\tau / s^2(l)$ (trend stationarity).

The asymptotic distributions are

- $\eta_\mu \to \sigma^2 \int_0^1 V(r)^2 dr$.
- $\eta_\tau \to \sigma^2 \int_0^1 V_2(r)^2 dr$.

Here $V(r) = W(r) - r W(1)$ is a Brownian bridge with $V(r) \sim N(0, r(1-r))$ on $[0,1]$.

Simulations give critical values for $\int_0^1 V(r)^2 dr$ and $\int_0^1 V_2(r)^2 dr$, leading to the KPSS table:

| Statistic & quantile | 0.10 | 0.05 | 0.025 | 0.01 |
| :--- | :---: | :---: | :---: | :---: | :---: |
| **$\eta_\mu$**: $\int_0^1 V(r)^2 dr$ | 0.347 | 0.463 | 0.574 | 0.739 |
| **$\eta_\tau$**: $\int_0^1 V_2(r)^2 dr$ | 0.119 | 0.146 | 0.176 | 0.216 |

By comparing $\hat{\eta}_\mu$ or $\hat{\eta}_\tau$ to these critical values, we obtain the p‑value and decide whether to reject stationarity.

### CH Test

The Canova–Hansen (CH) test is a LM‑type test for **seasonal unit roots** and **non‑constant seasonal patterns**.

Model:

$$
y_t = \mu + x_t' \beta + S_t + \varepsilon_t,
$$

where

- $x_t, \beta \in \mathbb{R}^k$ are regressors and coefficients.
- $\varepsilon_t \sim N(0, \sigma^2)$ may be heteroscedastic.
- $S_t$ is a deterministic seasonal component with period $s$ (odd integer).

Two ways to model $S_t$:

- Using $s$ seasonal dummies $S_t = d_t' \alpha$:
    - $d_t \in \mathbb{R}^s$ are seasonal dummies.
    - $\alpha \in \mathbb{R}^s$ are coefficients.
- Using $2q + 1$ trigonometric frequencies $S_t = f_t' \gamma = \sum_{j=1}^q f_{jt}' \gamma_j$ with $q = s/2$:
    - $\gamma \in \mathbb{R}^q$ are coefficients.
    - Trigonometric terms

      $$
      f_{jt} =
      \begin{cases}
      (\cos(2 \pi j t / s), \sin(2 \pi j t / s)), & j < q, \\
      (\cos(\pi t)), & j = q.
      \end{cases}
      $$

To ensure test power, we require:

- The tested series $y_t$ should not contain non‑seasonal unit roots; otherwise CH loses power.
- If $x_t$ includes lags $y_{t-1}, y_{t-2}, ...$, the AR part should not absorb seasonal patterns; usually we limit this to first‑order lags.

Rewriting the regressions:

- Dummy seasonal model: $y_t = x_t' \beta + d_t' \alpha + e_t$.
- Trigonometric seasonal model: $y_t = \mu + x_t' \beta + f_t' \gamma + e_t$.

After auxiliary regression, the residuals $\hat{e}_t$ are (asymptotically) the same in both specifications.

#### Robust Covariance Estimation

Let

- $D_n = [d_1 e_1, ..., d_n e_n]$, and $\Omega = \lim_{n \to \infty} n^{-1} E(D_n D_n')$.
- $F_n = [f_1 e_1, ..., f_n e_n]$, and $\Omega^f = \lim_{n \to \infty} n^{-1} E(F_n F_n')$.

Due to seasonality, $\hat{e}_t$ is often heteroscedastic and serially correlated, so we use robust estimators:

- $\hat{\Omega} = \sum_{k=-m}^m w(k/m) \frac{1}{n} \sum_i d_{i+k} \hat{e}_{i+k} d_i' \hat{e}_i$.
- $\hat{\Omega}^f = \sum_{k=-m}^m w(k/m) \frac{1}{n} \sum_i f_{i+k} \hat{e}_{i+k} f_i' \hat{e}_i$.

Here $w(\cdot)$ is a kernel ensuring positive semidefiniteness.

#### Seasonal Unit Root Test

Assume the trigonometric coefficients follow a random walk

$$
\gamma_t = \gamma_{t-1} + u_t.
$$

If $E(u_t u_t')$ is full rank, all frequencies in $y_t$ have seasonal unit roots.

Let $A \in \mathbb{R}^{(s-1) \times a}$ be a full‑rank selection matrix to choose $a$ frequencies of interest:

- $A = I_{s-1}$: test all of $\gamma$.
- $A = (\tilde{0}, 1)$: test the last frequency $\pi$ (period 2).
- $A = (\tilde{0}, I_2, \tilde{0})$: test a specific frequency $j \pi / q$.

The random walk can be written as

$$
A' \gamma_t = A' \gamma_{t-1} + u_t,
$$

with $E(u_t u_t') = \tau^2 G$, where $\tau^2 \ge 0$ and $G = (A' \Omega^f A)^{-1}$.

- $\tau^2 = 0$: coefficients are constant ($\gamma_t = \gamma_0$).
- $\tau^2 > 0$: the selected seasonal frequencies have unit roots.

CH hypotheses:

- $H_0: \tau^2 = 0$ (no seasonal unit roots).
- $H_1: \tau^2 > 0$ (seasonal unit roots present).

The LM statistic is

$$
L = \frac{1}{n^2} (A' \hat{\Omega}^f A)^{-1} A' \sum_{i=1}^n \hat{F}_i \hat{F}_i' A.
$$

Under $H_0$, $L$ converges to a von Mises distribution $\text{VM}(a)$, where $a = \text{rank}(A)$.

#### Non‑constant Seasonal Pattern Test

Similarly, for the dummy seasonal model with $\alpha_t$ following

$$
A' \alpha_t = A' \alpha_{t-1} + u_t,
$$

and $E(u_t u_t') = \tau^2 G$, we have

- $\tau^2 = 0$: constant seasonal pattern ($\alpha_t = \alpha_0$).
- $\tau^2 \ne 0$: non‑constant seasonal pattern (e.g., random walk or occasional breaks).

CH now tests

- $H_0: \tau^2 = 0$ (constant seasonal pattern).
- $H_1: \tau^2 \ne 0$ (non‑constant seasonal pattern).

The LM statistic is

$$
L = \frac{1}{n^2} (A' \hat{\Omega} A)^{-1} A' \sum_{i=1}^n \hat{D}_i \hat{D}_i' A,
$$

with the same $\text{VM}(a)$ asymptotic distribution.

#### von Mises Distribution

Critical values for $\text{VM}(p)$ with degrees of freedom $p$ are:

| p | 1% | 2.5% | 5% | 7.5% | 10% | 20% |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 1 | .748 | .593 | .470 | .398 | .353 | .243 |
| 2 | 1.070 | .898 | .749 | .670 | .610 | .469 |
| 3 | 1.350 | 1.160 | 1.010 | .913 | .846 | .679 |
| 4 | 1.600 | 1.390 | 1.240 | 1.140 | 1.070 | .883 |
| 5 | 1.880 | 1.630 | 1.470 | 1.360 | 1.280 | 1.080 |
| 6 | 2.120 | 1.890 | 1.680 | 1.580 | 1.490 | 1.280 |
| 7 | 2.350 | 2.100 | 1.900 | 1.780 | 1.690 | 1.460 |
| 8 | 2.590 | 2.330 | 2.110 | 1.990 | 1.890 | 1.660 |
| 9 | 2.820 | 2.550 | 2.320 | 2.190 | 2.100 | 1.850 |
| 10 | 3.050 | 2.760 | 2.540 | 2.400 | 2.290 | 2.030 |
| 11 | 3.270 | 2.990 | 2.750 | 2.600 | 2.490 | 2.220 |
| 12 | 3.510 | 3.180 | 2.960 | 2.810 | 2.690 | 2.410 |

Response surface regressions can be used to obtain p‑values for arbitrary sample sizes and seasonal periods, e.g.:

https://github.com/GeoBosh/uroot/blob/master/R/ch-rs-pvalue.

## ARMA and ARIMA Models

An autoregressive moving average (ARMA) model combines both lags and past errors:

$$
\text{ARMA}(p, q):\quad y_t = c + \phi_1 y_{t-1} + \cdots + \phi_p y_{t-p} + \theta_1 \varepsilon_{t-1} + \cdots + \theta_q \varepsilon_{t-q} + \varepsilon_t,
$$

- $\phi_i$ are AR coefficients.
- $\theta_i$ are MA coefficients.
- $\varepsilon_t$ is white noise.

The AR coefficients must satisfy stationarity and the MA coefficients must satisfy invertibility.

ARMA models are less interpretable than pure AR or MA models, but they can capture rich patterns in time series.

Often we difference the data before fitting ARMA to remove trends. Incorporating differencing into the model yields the ARIMA (Autoregressive Integrated Moving Average) model $\text{ARIMA}(p, d, q)$, where $d$ is the differencing order.

- $\text{ARIMA}(0, 0, 0), c = 0 \ \to\ \text{WN}$.
- $\text{ARIMA}(0, 1, 0), c = 0 \ \to\ \text{RW}$.
- $\text{ARIMA}(0, 1, 0), c \ne 0 \ \to\ \text{RWD}$.
- $\text{ARIMA}(p, 0, q) \ \to\ \text{ARMA}(p, q)$.

The long‑run forecast behavior depends on the constant $c$ and differencing order $d$:

- $c = 0, d = 0$: forecasts converge to 0.
- $c = 0, d = 1$: forecasts converge to a nonzero constant.
- $c = 0, d = 2$: forecasts show a linear trend.
- $c \ne 0, d = 0$: forecasts converge to the mean.
- $c \ne 0, d = 1$: forecasts show a linear trend.
- $c \ne 0, d = 2$: forecasts show a quadratic trend.

The larger $d$ is, the faster the prediction interval widens:

- $d = 0$: forecast standard deviation equals that of historical data.
- $d = 1$: forecast standard deviation grows linearly with horizon.
- $d = 2$: forecast standard deviation grows faster than linearly.

Using the backshift operator,

$$
y_t = c + \phi_1 B y_t + \cdots + \phi_p B^p y_t + \theta_1 B \varepsilon_t + \cdots + \theta_q B^q \varepsilon_t + \varepsilon_t.
$$

Rearranging,

$$
(1 - \phi_1 B - \cdots - \phi_p B^p) y_t = c + (1 + \theta_1 B + \cdots + \theta_q B^q) \varepsilon_t.
$$

Let

$$
\phi(B) = 1 - \phi_1 B - \cdots - \phi_p B^p, \quad \theta(B) = 1 + \theta_1 B + \cdots + \theta_q B^q,
$$

then

$$
\phi(B) y_t = c + \theta(B) \varepsilon_t.
$$

For $\text{ARIMA}(p, d, q)$, apply differencing $y_t' = (1 - B)^d y_t$:

$$
(1 - \phi_1 B - \cdots - \phi_p B^p) (1 - B)^d y_t = c + (1 + \theta_1 B + \cdots + \theta_q B^q) \varepsilon_t.
$$

That is,

$$
\phi(B) (1 - B)^d y_t = c + \theta(B) \varepsilon_t,
$$

with typically $d \le 2$.

For seasonal data, a simple approach is seasonal differencing $(1 - B^m) y_t$ with period $m$.

A more general approach is the seasonal ARIMA model $\text{SARIMA}(p, d, q)(P, D, Q)_m$, which adds seasonal differencing $(1 - B^m)^D$ and seasonal AR/MA polynomials $\Phi(B^m)$, $\Theta(B^m)$:

$$
\phi(B) \Phi(B^m) (1 - B)^d (1 - B^m)^D y_t = c + \theta(B) \Theta(B^m) \varepsilon_t,
$$

usually with $D \le 1$ and $P, Q \le 3$.

Traditional seasonal decomposition assumes the seasonal pattern repeats identically each cycle. SARIMA, in contrast, allows randomness in the seasonal pattern and models it via $\Phi(B^m)$ and $\Theta(B^m)$.

Note that SARIMA is not suitable for multiple seasonalities or very large seasonal periods.

Define

- $\Delta^d \Delta_m^D y_t = (1 - B)^d (1 - B^m)^D y_t$,
- $\phi^*(B)$ as the combined AR polynomial of order $p + mP$,
- $\theta^*(B)$ as the combined MA polynomial of order $q + mQ$.

Then

$$
\phi^*(B) \Delta^d \Delta_m^D y_t = c + \theta^*(B) \varepsilon_t,
$$

so $\Delta^d \Delta_m^D y_t$ follows an $\text{ARMA}(p + mP, q + mQ)$ process with some zero coefficients. Hence SARIMA and ARMA can share the same likelihood machinery.

### Properties

If we view an ARMA model as an operator $\mathcal{F}(\cdot)$ on the input $x(t)$ (the past values $y_{t-1}, ..., y_1$), then ARMA defines a linear system:

$$
\mathcal{F}[\lambda_1 x_1(t) + \lambda_2 x_2(t)] = \lambda_1 \mathcal{F}[x_1(t)] + \lambda_2 \mathcal{F}[x_2(t)].
$$

If the parameters are constant, the mapping does not change over time, so ARMA is **time‑invariant**:

$$
y(t) = \mathcal{F}[x(t)] \ \Rightarrow\ y(t-\tau) = \mathcal{F}[x(t-\tau)].
$$

If input and output share the same distribution, the system is **stable**.

If the output depends only on current and past inputs and not on future inputs, the system is **causal**.

Define

- $\phi(z) = 1 - \phi_1 z - \cdots - \phi_p z^p$.
- $\theta(z) = 1 - \theta_1 z - \cdots - \theta_q z^q$.

Then

- Stationarity: $\phi(z) \ne 0$ for $|z| = 1$.
- Causality: $\phi(z) \ne 0$ for $|z| \le 1$.
- Invertibility: $\theta(z) \ne 0$ for $|z| \le 1$.

For a causal and invertible ARMA$(p, q)$, we can write

$$
y_t = \sum_{i=0}^\infty \psi_i \varepsilon_{t-i},
$$

with

$$
\psi_i =
\begin{cases}
1, & i = 0, \\
\theta_i + \sum_{j=1}^{\min(p, i)} \phi_j \psi_{i-j}, & i \ge 1.
\end{cases}
$$

Because $\varepsilon_t \overset{iid}{\sim} N(0, \sigma_\varepsilon^2)$,

$$
Var(y_t) = \sum_{j=0}^\infty Var(\psi_j \varepsilon_{t-j}) = \sigma_\varepsilon^2 \sum_{j=0}^\infty \psi_j^2.
$$

For a causal ARMA$(p, q)$, let $m = \max(p, q)$ and define

$$
w_t =
\begin{cases}
\sigma^{-1} y_t, & t = 1, ..., m, \\
\sigma^{-1} \phi(B) y_t, & t > m.
\end{cases}
$$

Then the autocovariance of $w_t$ is

$$
\gamma_{w(i,j)} =
\begin{cases}
\sigma^{-2} \gamma_{y(i-j)}, & 1 \le i, j \le m, \\
\sigma^{-2} [\gamma_{y(i-j)} - \sum_{r=1}^p \phi_r \gamma_{y(r - |i-j|)}], & \min(i, j) \le m < \max(i, j) \le 2m, \\
\sum_{r=1}^q \theta_r \theta_{r + |i-j|}, & \min(i, j) > m, \\
0, & \text{otherwise}.
\end{cases}
$$

Given $\gamma_w$, we can apply the innovations algorithm to obtain $\theta_{t,i}$ and $r_t$:

- Innovations equation

    $$
    \hat{w}_{t+1} =
    \begin{cases}
    \sum_{i=0}^t \theta_{t,i} (w_{t+1-i} - \hat{w}_{t+1-i}), & 1 \le t < m, \\
    \sum_{i=0}^q \theta_{t,i} (w_{t+1-i} - \hat{w}_{t+1-i}), & t \ge m.
    \end{cases}
    $$

- Forecast variance $r_t = E[(w_{t+1} - \hat{w}_{t+1})^2]$.

The error variance satisfies

$$
v_t = E[(y_{t+1} - \hat{y}_{t+1})^2] = \sigma_\varepsilon^2 E[(w_{t+1} - \hat{w}_{t+1})^2] = \sigma_\varepsilon^2 r_t.
$$

### Forecast Error

For ARMA$(p, q)$, the $l$‑step‑ahead model is

$$
y_{t+l} = c + \phi_1 y_{t+l-1} + \cdots + \phi_p y_{t-p+l} + \varepsilon_{t+l} + \theta_1 \varepsilon_{t+l-1} + \cdots + \theta_q \varepsilon_{t+l-q}.
$$

The conditional expectation is

$$
\begin{aligned}
\hat{y}_{t+l|t} &= E(y_{t+l} | I_t) \\
&= c + E(\varepsilon_{t+l} | I_t) \\
&\quad + E(\phi_1 y_{t+l-1} + \cdots + \phi_p y_{t-p+l} | I_t) \\
&\quad + E(\theta_1 \varepsilon_{t+l-1} + \cdots + \theta_q \varepsilon_{t+l-q} | I_t).
\end{aligned}
$$

Given $I_t = \{y_t, y_{t-1}, ..., y_{t-p}\}$:

- $E(\varepsilon_{t+l}) = 0$.
- $E(\phi_1 y_{t+l-1} + \cdots + \phi_p y_{t-p+l}) = \sum_{i=1}^p \phi_i \hat{y}_{t+l-i|t}$.
- $E(\theta_1 \varepsilon_{t+l-1} + \cdots + \theta_q \varepsilon_{t+l-q}) = \sum_{i=1}^q \theta_i \varepsilon_{t+l-i}$ for $l \le q$ and 0 for $l > q$.

Forecast errors:

- One‑step:

    $$
    e_t(1) = y_{t+1} - \hat{y}_{t+1|t} = \varepsilon_{t+1}.
    $$

- Two‑step (ARMA(1, 1) as illustration):

    $$
    e_t(2) = (\phi_1 + \theta_1) \varepsilon_{t+1} + \varepsilon_{t+2}.
    $$

- Three‑step:

    $$
    e_t(3) = (\phi_1 (\phi_1 + \theta_1) + \phi_2 + \theta_2) \varepsilon_{t+1} + (\phi_1 + \theta_1) \varepsilon_{t+2} + \varepsilon_{t+3}.
    $$

In general,

$$
e_t(l) = y_{t+l} - \hat{y}_{t+l|t} = \sum_{i=1}^l \psi_i \varepsilon_{t+l-i} + \varepsilon_{t+l},
$$

where

$$
\psi_j =
\begin{cases}
-1, & j = 0, \\
-\Big(\sum_i^j \phi_i \psi_{j-i}\Big) + \theta_j, & 1 \le j \le q, \\
-\sum_i^j \phi_i \psi_{j-i}, & j > q.
\end{cases}
$$

The variance is

$$
Var(e_t(l)) = (\psi_1^2 + \psi_2^2 + \cdots + \psi_{l-1}^2 + 1) \sigma_\varepsilon^2.
$$

### Parameter Estimation and Likelihood

One common approach is conditional sum of squares (CSS), followed by maximum likelihood using numerical optimization. A reference implementation can be found in R’s `arima` source code, e.g.:

https://github.com/SurajGupta/r-source/blob/a28e609e72ed7c47f6ddfbb86c85279a0750f0b7/src/library/stats/src/arima.c#L753

https://github.com/SurajGupta/r-source/blob/master/src/library/stats/R/arima.R#L248

Given ARMA$(p, q)$, the one‑step forecast is

$$
\hat{y}_{t+1} =
\begin{cases}
\sum_{j=1}^q \theta_j (y_{t+1-j} - \hat{y}_{t+1-j}), & 1 \le t < \max(p, q), \\
\sum_{j=1}^p \phi_j y_{t+1-j} + \sum_{j=1}^q \theta_j (y_{t+1-j} - \hat{y}_{t+1-j}), & t \ge \max(p, q).
\end{cases}
$$

Let $v_t = E[(y_{t+1} - \hat{y}_{t+1})^2]$.

Conditionally,

$$
y_t | y_{t-1}, ..., y_1 \sim N(\hat{y}_t, v_t),
$$

with density

$$
P(y_t | y_{t-1}, ..., y_1) = \frac{1}{\sqrt{2 \pi v_t}} \exp\Big(-\frac{(y_t - \hat{y}_t)^2}{2 v_t}\Big).
$$

Let $v_t = \sigma^2 r_t$ and $S(\phi, \theta) = \sum_{t=1}^n \frac{(y_t - \hat{y}_t)^2}{r_{t-1}}$. The likelihood is

$$
L(\phi, \theta, \sigma^2) = (2 \pi \sigma^2)^{-n/2} \Big(\prod_{t=1}^n r_{t-1}\Big)^{-1/2} \exp\Big(-\frac{S(\phi, \theta)}{2 \sigma^2}\Big).
$$

The log‑likelihood is

$$
-2 \ell(\phi, \theta, \sigma^2) = n \log(2 \pi \sigma^2) + \sum_{t=1}^n \log r_{t-1} + \frac{S(\phi, \theta)}{\sigma^2}.
$$

Differentiating w.r.t. $\sigma^2$ gives

$$
\hat{\sigma}^2 = \frac{S(\hat{\phi}, \hat{\theta})}{n}.
$$

Substituting and dropping constants yields the concentrated log‑likelihood

$$
\ell(\phi, \theta) = \log\Big(\frac{S(\phi, \theta)}{n}\Big) + \frac{1}{n} \sum_{t=1}^n \log r_{t-1}.
$$

$\ell(\phi, \theta)$ has no closed form solution, so numerical optimization is used.

From the likelihood we can compute information criteria:

- $AIC = -2 \log L + 2(k + 1)$.
- $BIC = -2 \log L + \log(n)(k + 1)$.
- $AIC_c = AIC + \dfrac{2(k + 1)(k + 2)}{n - k - 2}$.

with

$$
k =
\begin{cases}
p + q + P + Q, & c = 0, \\
p + q + P + Q + 1, & c \ne 0.
\end{cases}
$$

### Modeling Workflow

Two main workflows:

- Box–Jenkins.
- Step‑wise.

**Box–Jenkins** (manual, mainly for AR and MA):

1. Inspect and transform data.
    - Apply log or Box–Cox to stabilize variance.
    - Difference to remove deterministic trends.
2. Plot ACF and PACF.
    - MA$(q)$: ACF cuts off at lag $q$.
    - AR$(p)$: PACF cuts off at lag $p$.
3. Estimate parameters using Yule–Walker, Burg, etc.
4. Select model via AIC.

**Step‑wise** (automatic, for general ARIMA/SARIMA):

- Use CH test to choose seasonal differencing order $D$.
- Use KPSS to choose non‑seasonal differencing order $d$.
- Form stationary $\nabla y_t = (1 - B)^d (1 - B^m)^D y_t$.
- Fit ARMA$(p + mP, q + mQ)$ to $\nabla y_t$.
- Explore different $p, q, P, Q$ and select via information criteria.
- Check residuals via Ljung–Box tests.

A practical reference:

https://otexts.com/fpp3/arima-r.html

### Modeling Residuals

In standard linear regression we assume white noise errors. In practice, residuals $e = y - \hat{y}$ often show autocorrelation.

We then model the error term $\eta_t$ as a zero‑mean stationary process, e.g. an ARMA$(p, q)$:

$$
\begin{aligned}
y_t &= \beta_1 x_{1,t} + \cdots + \beta_k x_{k,t} + \eta_t, \\
\phi(B) \eta_t &= \theta(B) \varepsilon_t, \\
\varepsilon_t &\sim \text{WN}(0, \sigma^2).
\end{aligned}
$$

If $\eta_t$ is non‑stationary, we can difference $y_t, x_t, \eta_t$ and use ARIMA$(p, d, q)$ instead:

$$
\begin{aligned}
y_t' &= \beta_1 x_{1,t}' + \cdots + \beta_k x_{k,t}' + \eta_t', \\
\phi(B) \eta_t' &= \theta(B) \varepsilon_t.
\end{aligned}
$$

OLS and GLS estimates are

- OLS (ignoring error correlation):

    $$
    \boldsymbol{\hat{\beta}}_{\text{OLS}} = (X' X)^{-1} X' y, \quad Cov(\boldsymbol{\hat{\beta}}_{\text{OLS}}) = (X' X)^{-1} X' \Gamma_n X (X' X)^{-1},
    $$

    where $\Gamma_n = E(\boldsymbol{\eta \eta'})$.

- GLS (using $\Gamma_n^{-1}$ as weight):

    $$
    \boldsymbol{\hat{\beta}}_{\text{GLS}} = (X' \Gamma_n^{-1} X)^{-1} X' \Gamma_n^{-1} y, \quad Cov(\boldsymbol{\hat{\beta}}_{\text{GLS}}) = (X' \Gamma_n^{-1} X)^{-1}.
    $$

GLS provides the best linear unbiased estimator if $\Gamma_n$ is known. In practice, $\Gamma_n$ is built from the fitted ARMA$(p, q)$ model for $\eta_t$.
