---
title: 统计学基础知识
cover: '/cover/Galton-Board-TShirt.png'
date: 2025-07-22
tags: ['统计']
---

统计学是机器学习 (ML) 和人工智能 (AI) 的核心基石。它提供了理解数据、发现模式和量化不确定性的基本工具。
从数据的预处理、模型训练到评估，ML 和 AI 的每一个环节都离不开统计学，它为这些技术提供了理论支撑和方法论指导。

[comment]:summary


## 基本概念

### 相关术语

**样本空间（Ω）** 是指一个随机实验所有可能结果的完整集合：

- 抛掷一个骰子： $\Omega_{\text{dice}}=\{1,2,3,4,5,6\}$
- 磁盘故障间隔： $\Omega_{\text{MTBF}}=[0,∞)$
- 服务每秒请求： $\Omega_{\text{QPS}}=\{1,2,3,...,∞\}$
- 日内股价变化： $\Omega_{\text{return}}=[-100\%,∞)$


**随机变量（X : Ω → R）** 是一个函数，它将样本空间中的每一个基本结果映射到一个实数：

- 抛骰子结果的点数为 5 ： $X_{\text{dice}} = 5$
- 磁盘持续运行超过 10 万小时不发生故障 ： $X_{\text{MTBF}} > 100\text k$
- 服务的在某段时间的 QPS 为 10k ： $X_{\text{QPS}} = 10\text k$
- 股价下跌 ：$X_{\text{return}} < 0$


 **事件** 是样本空间的一个子集，用于描述我们感兴趣的结果及其对应的概率：

- “掷骰子得到偶数”
  - $\{2,4,6\} \supset\Omega_{\text{dice}}$
  - $P(X\text{ is odd})$
- “掷骰子得到小于3的点”
  - $\{1,2\} \supset\Omega_{\text{dice}}$
  - $P(X<3)$
- “磁盘持续无故障运行超过 10 万小时” 
  - $[100\text k,∞) \supset\Omega_{\text{dice}}$
  - $P(X > 100\text k)$
- “磁盘在运行 5 万到 8 万小时后发生故障” 
  - $[50\text k,80\text k] \supset\Omega_{\text{dice}}$
  - $P(50\text k ≤ X ≤ 80\text k)$

### 概率分布

样本空间包含了所有可能性，因此整个样本空间的概率之和为 $1$。\
事件是样本空间的一个子集，因此某个事件发生的概率范围为 $[0,1]$。

根据样本空间是否可数，可以将随机变量分为两类：
- 离散型随机变量：样本空间是可数的，可以计算某个样本的概率\
  例如：$P(X_{\text{dice}} = 5) = 1/6$
- 连续型随机变量：样本空间是不可数的，只能计算某个区间的概率\
  例如：$P(X_{\text{return}} < 0) = 50\%$


当样本空间由可数样本点 **ω** 组成时 ，某个事件（子集）发生的概率计算方式如下：
- 遍历子集中的每个样本点 $\omega$
- 计算每个样本点对应的概率 $P(\omega)$
- 对所有样本点的概率求和 $∑P(\omega)$

但以上朴素的事件概率计算公式仅适用于离散型变量，不适用于连续性随机变量。\
为了统一事件概率的计算方式，我们需引入概率分布的概念。

概率分布是一组函数，描述了一个随机变量 $X$ 在各个可能取值上出现的概率。\
当使用随机变量表示事件时，我们可以通过概率分布计算事件发生的概率。


概率分布本质上是一些日常生活中总结出的规律：

- 假设骰子质地分布均匀，掷骰子的结果服从 **均匀分布**\
  [$X_{\text{dice}} \sim \text{Uniform}(a,b)$](650x450:tool/distribution-curve/uniform?curve1=1,6,212121,dice)，
  其中 a 与 b 分别对应最大与最小点数

- 假设故障率与使用时间无关，磁盘故障间隔服从 **指数分布**\  
  [$X_{\text{MTBF}} \sim \text{Exp}(\lambda)$](650x450:/tool/distribution-curve/exponential?curve1=0.0001,212121,MTBF)，
  其中 λ 是平均故障率

- 假设每个请求之间互不相关，服务的 QPS 服从 **泊松分布**\
  [$X_{\text{QPS}} \sim \text{Poisson}(\lambda)$](650x450:/tool/distribution-curve/poisson?curve1=10,212121,QPS)，
  其中 λ 是 QPS 均值

- 假设股价变化不可预测（随机游走），对数股价变化率服从 **正态分布**\
  [$\ln(X_{\text{return}}) ~ \text{Normal}(\mu,\sigma)$](650x450:/tool/distribution-curve/normal?curve1=0.1,2,212121,Δln(S))，
  其中 μ 是昨收价，σ 是波动率，其中：\
  $\ln(X_{\text{return}}) =\ln (S_{t+1}/S_t) = \ln S_{t+1} − \ln S_t = ε_{t+1} \sim \text{Normal}(\mu,\sigma) $

### 分布函数

为了准确描述随机变量的概率分布，需要借助 3 个核心函数

- **概率质量函数 $p(x)$（Probability Mass Function，简称 PMF）** 描述离散随机变量在特定取值上的概率的函数：
  - [$p_{X_{\text{dice}}}(x) = \text{Uniform}(1,6)$](650x450:/tool/distribution-curve/uniform?curve1=1,6,212121,dice)
  - [$p_{X_{\text{QPS}}}(x) = \text{Poisson}(10)$](650x450:/tool/distribution-curve/poisson?curve1=10,212121,QPS)

- **概率密度函数 $f(x)$（Probability Density Function，简称 PDF）** 描述连续随机变量在特定取值点附近的可能性的函数：
  - [$f_{X_{\text{MTBF}}} = \text{Exp}(1/10000) $](650x450:/tool/distribution-curve/exponential?curve1=0.0001,212121,MTBF)
  - [$f_{X_{\text{log-return}}} = \text{Normal}(100,0.1)$](650x450:/tool/distribution-curve/normal?curve1=0.1,2,212121,Δln(S))

- **累积分布函数 $F(x)$（Cumulative Distribution Function，简称 CDF）** 描述随机变量取值小于或等于某个特定值的概率的函数：
  - $P(X≤x)=F_X(x)$           随机变量 $X$ 小于或等于 $x$ 的概率
  - $P(X>x)=1−F_X(x)$         随机变量 $X$ 大于 $x$ 的概率
  - $P(a<X≤b)=F_X(b)-F_X(a)$  随机变量 $X$ 取值落在区间 $(a,b]$ 的概率


其中，CDF 的逆函数 $F_X^{-1}(x)$ 被称为 **逆累积分布函数（Inverse CDF）** 或 **百分位函数（Quantile Function）**。\
用于计算随机变量 $X$ 小于某个特定概率 $p$ 时的取值 $x = F_X^{-1}(p) \to P(X≤x) = p$。\
虽然其并不直接描述概率分布，但在假设检验中扮演重要角色。


### 概率性质

在实际应用中，我们主要考察随机变量的两个核心性质：

- **期望（Expectation）**：随机变量所有可能取值与其对应概率的乘积之和（加权均值）
  - 连续型随机变量：$E(X)=∫xf(x)dx$
  - 离散型随机变量：$E(X)=∑_ix_ip(x_i)$

- **方差（Variance）**：随机变量所有可能取值与期望之间的平均距离（不确定性）
  - 连续型随机变量：$Var(X)=E[(X−E(X))^2]=∫(x−μ)^2f(x)dx$
  - 离散型随机变量：$Var(X)=E[(X−E(X))^2]=∑(x_i−μ)^2 p(x_i)$


如果某个随机变量的概率分布已知，则对应的方差与均值可以直接由概率分布给出。

以 $\text{Uniform}(a,b)$ 为例，其期望与方差由以下公式给出：
- $E(X)=\frac{a+b}2 $
- $Var(X)=\frac{(b-a+1)^2-1}{12}$

因此随机变量 $X_\text{dice} \sim \text{Uniform}(1,6)$ 的期望与方差为：
- $E(X_\text{dice})=\frac{1+6}2=3.5$
- $Var(X_\text{dice})=\frac{(6-1+1)^2-1}{12}=3$


### 抽样调查

在实践中，我们需要处理以下情况：
- 随机变量的概率分布未知
- 样本空间过大导致无法普查

为了研究随机变量的性质，只能从 **总体（Population）** 中抽取 **样本（Sample）** 进行研究：
- 总体均值：$$\mu = \frac{1}{N}\sum^N_ix_i$$
- 总体方差：$$\sigma^2 = \frac{1}{N}\sum^n_i(x_i -\mu)^2$$
- 样本均值：$$\bar{x} = \frac{1}{n}\sum^n_ix_i$$
- 样本方差（有偏估计）：$$\hat{\sigma^2} = \frac{1}{n}\sum^n_i(x_i -\bar{x})^2$$
- 样本方差（无偏估计）：$$s^2 = \frac{1}{n-1}\sum^n_i(x_i -\bar{x})^2$$

抽样调查的目的是为了得到一个尽可能接近总体 $\mu,\sigma^2$ 的估计值 $\bar x,s^2$。\
当抽样次数 $m$ 足够多时，样本均值能够逼近总体均值 ：$\mu = \frac{1}{m}\sum^m_{i}X_i\ (m \to \infty)$。

总体的数量 $N$ 的数量是一个很大的值，而 $n$ 则是一次抽样调查中的样本数量。\
当 $n \ll N $ 时，抽中极端值的概率较小，从而低估方差，导致 $\hat{\sigma^2}$ 偏小。\
当样本数量较少时，通常使用无偏估计 $s^2$ 作为 $\sigma^2$ 近似。


## 正态分布

### 中心极限定理

借助随机变量的概念，我们可以通过以下方式对样本数据进行研究：
- 假设总体属于某个特定分布
- 根据样本估计该分布的参数
- 基于概率分布函数进行分析

> 当没有足够的先验知识时，通常会假设随机变量服从正态分布

该假设的理论基石是统计学中的 **渐近正态性（Asymptotic Normality）**：
> 在样本量趋于无穷大时，随机变量的分布趋近于正态分布

渐近正态性的一个重要特例是 **中心极限定理（Central Limit Theorem, CLT）**：
> 对于服从任意分布的的随机变量，只要抽样次数足够多，其均值总会服从正态分布

为了方便大家理解，这里提供了一个小实验，大家可以动手调整采样次数，并观察分布变化：

- [均匀分布抽样](700x700:/tool/clt-visualizer?dist=uniform(1,6)&sampleCount=1000&sampleSize=100)
- [指数分布抽样](700x700:/tool/clt-visualizer?dist=exponential(0.1)&sampleCount=1000&sampleSize=100)


渐近正态性与中心极限定理是众多统计学应用的基石：
> 基于样本均值和标准差，可以构建总体均值的置信区间，即使总体分布未知

这使得我们能够对许多非正态分布的数据进行基于正态分布的统计推断。

中心极限定理的公式化描述如下（这部分可以跳过）：

- 给定一个服从任意分布的的随机变量 $X$。
- 使用 $$Y_n$$ 表示对 $X$ 抽样 $n$ 次后的均值 $$Y_n = \frac{1}{n}\sum^n_{i=1}X_i$$，其对应的统计量为：
  - 期望 $$E[Y_n] = E[\frac{1}{n}\sum^n_{i=1}X_i] = \frac{1}{n}\sum^n_{i=1}E[X_i] = \frac{1}{n}nE[X] = E[X]$$
  - 方差 $$Var[Y_n] = Var[\frac{1}{n}\sum^n_{i=1}X_i] = \frac{1}{n^2}\sum^n_{i=1}Var[X_i] = \frac{1}{n^2}nVar(X) = \frac{Var(X)}{n}$$

- 当 $n > 30$ 时，$$Y_n$$ 服从正态分布 $$Y_n \sim \mathcal{N}(E[X],\frac{Var(X)}{n})$$，标准化后 $$\frac{Y_n-E[X]}{\sqrt{Var(X)/n}} \sim \mathcal{N}(0,1)$$。


### 正态分布

**[正态分布（Normal Distribution）](650x450:/tool/distribution-curve/normal)** 是概率论和统计学中最重要、最常见的连续概率分布之一。\
它的概率密度函数曲线呈现出独特的**钟形曲线（Bell Curve）**。\
大多数数据点都集中在均值附近，距离均值越远的数据点，其出现的概率越低。

![Empirical-Rule?width=700](/picture/how-statistics-work/empirical_rule.svg)


正态分布由两个参数完全决定：
- **均值（Mean，μ）**：决定了分布的中心位置
- **标准差（Standard Deviation，σ）**：决定了分布的“宽度”或分散程度
  - 标准差越大，数据越分散，钟形曲线就越扁平、越宽
  - 标准差越小，数据越集中，钟形曲线就越高耸、越窄


正态分布衍生出了 3 个统计学中常用的分布：
- **Z 分布**

    任何一个正态分布随机变量 $X$ 都可以转化为标准正态分布 $Z = \frac{(X−μ)}σ \sim \mathcal N(0,1)$\
    该随机变量的取值被称为 z-score ，表示原始数据点 X 距离其均值 μ 有多少个 σ 的距离\
    后续我们将基于 z-score 构建置信区间


- **卡方分布**

    假设 $Z_1,...Z_k$ 是 $k$ 个相互独立且服从标准正态分布 $\mathcal N(0,1)$ 的随机变量\
    那么它们的平方和服从自由度为 $k$ 的卡方分布 $Z_1^2+\cdots+Z_k ^2 \sim χ^2(k)$\
    通常用于检验多个随机变量间的独立性（取值概率互不相关）与同质性（服从相同分布）


- **t 分布**

    给定 $Z \sim N(0,1)$ 与 $V \sim χ^2(k) $，两者比值服从自由度为 $k$ 的 t 分布 $T=\frac{Z}{\sqrt{V/k}} \sim t(k) $\
    主要用于在总体标准差未知且样本量较小时，对正态总体均值进行统计推断


### 经验法则

正态分布的核心价值在于 **经验法则（Empirical Rule）** ：
- 大约 68% 的数据点落在距均值 1 个标准差的范围内（即 $μ±σ$）
- 大约 95% 的数据点落在距均值 2 个标准差的范围内（即 $μ±2σ$）
- 大约 99.7% 的数据点落在距均值 3 个标准差的范围内（即 $μ±3σ$）


尽管经验法则只是一个近似，但在许多实际应用中，它足以提供有价值的洞察和决策支持。\
比如：一个的数据点落在均值 3 个标准差之外（即在 $μ−3σ$ 以下或 $μ+3σ$ 以上）的概率小于 0.3%。\
因此可以基于 3-sigma 法则快速发现 **异常值（Outliers）**。

在实际应用中，通常使用 6-sigma 而不是简单的 3-sigma：
- 3-sigma 意味着 99.73% 的合格率，即每百万次机会 (DPMO) 有 2700 个缺陷
- 6-sigma 意味着 99.99966% 的合格率，这意味着每百万次机会仅有 3.4 个缺陷


在许多现代工业和服务领域，这个 3-sigma 的缺陷率是远远不能接受的：
- 医疗领域： 2700‱ 的医疗差错率，将导致大量患者受到伤害
- 航空航天： 2700‱ 的飞机部件缺陷，将是灾难性的
- 金融服务： 2700‱ 的交易错误，会造成巨大的经济损失和信任危机

只有 6-sigma 这种近乎零缺陷的追求，才符合当今对质量要求极高的市场环境。




## 置信区间

统计任务中，通常需要面对以下误差：
- 通过样本得到的估计均值 $\bar x$，与总体的实际均值 $\mu$ 之间存在误差
- 根据样本估计回归模型参数，估计值$\bar \beta$与实际值 $\beta$ 之间存在误差
- 基于估计参数进行回归预测，预测值$\bar y$与实际值$y$之间存在误差
由于总体的不可知性，因此这些误差是未知且无法消除的。


为了衡量某个统计任务的有效性，需要引入 [**置信区间（Confidence Interval）**](700x580:/tool/confidence-interval) 的概念：
- 将估计值看作正态分布的样本均值 $x̄$
- 以总体均值 $μ$ 为中心设置一个对称区间
- 计算样本均值 $x̄$ 落入该区间的概率

置信区间的两侧称为 **误差边界（Margin of Error）**：
- 样本均值 $x̄$ 落在误差边界外的概率称为 **显著水平（Significance Level）**，记为 $\alpha$
- 样本均值 $x̄$ 落在误差边界内的概率称为 **置信水平（Confidence Level）**，记为 $1-\alpha$

置信水平理解方式有两种：
- 以 $μ$ 为中心的置信区间，$x̄$ 出现在该区间内的概率
- 以 $x̄$ 为中心的置信区间，该区间内包含 $μ$ 的概率

在样本数量 $n$ 不变的情况下：
- [置信水平越高，置信区间越大，包含均值的概率更高，但是与均值的误差更大](700x580:/tool/confidence-interval?confLevel=90&sampleSize=1&sampleCount=30)
- [置信水平越低，置信区间越小，包含均值的概率更低，但是与均值的误差更小](700x580:/tool/confidence-interval?confLevel=50&sampleSize=1&sampleCount=30)

在置信水平 $1-\alpha$ 相同的情况下：
- [样本越多，均值分布的方差越小，样本均值与总体均值的误差越小](700x580:/tool/confidence-interval?confLevel=90&sampleSize=12&sampleCount=50)
- [样本越少，均值分布的方差越大，样本均值与总体均值的误差越大](700x580:/tool/confidence-interval?confLevel=90&sampleSize=2&sampleCount=50)


置信水平 $1-\alpha$ 的公式化表述（可以跳过）：

根据 CLT，样本均值$$\bar{x} $$服从正态分布 $$\bar{X} \sim \mathcal{N}(\mu,\frac{\sigma^2}{n})$$，标准化后得到 $$\frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}} \sim \mathcal{N}(0,1)$$
- 计算样本均值 $$\bar{x} $$与标准差 $$\frac{\sigma}{\sqrt{n}}$$（假设总体标准差 $$\sigma$$ 已知）
- 给定置信水平 $$1-\alpha$$
- 根据 $F_Z^{-1}(\frac{1-\alpha}2)$ 计算对应的 z-score $$z_{(1-\alpha)/2}$$ = $$\frac{\bar{x} - \mu}{\frac{\sigma}{\sqrt{n}}} $$
- 得到样本均值误差边界 $$\mu - z_{(1-\alpha)/2} \cdot \frac{\sigma}{\sqrt{n}} < \bar{x} < \mu + z_{(1-\alpha)/2} \cdot \frac{\sigma}{\sqrt{n}}$$
- 变换不等式得到 $$\bar{x} - z_{(1-\alpha)/2} \cdot \frac{\sigma}{\sqrt{n}} < \mu < \bar{x} + z_{(1-\alpha)/2} \cdot \frac{\sigma}{\sqrt{n}}$$

最终我们认为：有 $$1-\alpha$$ 的把握，认为均值 $$\mu$$ 位于区间  $$\bar{x} \pm z_{(1-\alpha)/2} \cdot \frac{\sigma}{\sqrt{n}}$$。


通常 $$\frac{\bar{x} - \mu}{\frac{\sigma}{\sqrt{n}}}$$ 中的总体标准差 $$\sigma$$ 无法得到，需要使用样本标准差无偏估计 $s$ 替代。\
得到的$$\frac{\bar{x} - \mu}{\frac{s}{\sqrt{n}}}$$ 不再服从 Z 分布，而是服从自由度为 $n-1$ 的学生 t 分布 $$\mathcal{t}_{n-1}$$。\
并将统计量替换为 t-score ，得到置信区间 $$\bar{x} \pm t_{(1-\alpha)/2} \cdot \frac{s}{\sqrt{n}}$$。


## 假设检验

假设检验是统计推断中一个非常重要的工具，它允许我们基于样本数据对总体参数做出判断或推论。\
简单来说，就是根据收集到的样本数据，来判断某个关于总体的假设是否成立。

样本统计量与总体实际值间始终存在误差，假设检验能够帮助我们在不确定性中做出数据驱动的决策。\
它提供了一个严谨的框架，用于评估观测到的现象是否仅仅是随机波动，还是真实存在的效应。


其核心思想是反证法：
- 先提出一个假设 $H_0$，然后收集样本数据
- 计算在该假设成立的情况下，出现样本数据的概率
- 如果概率非常小，意味着假设和现实不符，从而拒绝该假设


在假设检验中，需要提出两个互斥假设：
- **原假设 (Null Hypothesis, $H_0$)**：通常是研究者希望推翻的声明，或者说是“无效应”、“无差异”的声明
- **备择假设 (Alternative Hypothesis, $H_1$)**：通常是研究者希望证明的声明，与原假设相反

> 当 $H_0$ 被拒绝时，意味着 $H_1$ 成立

需要注意的是，$H_0$ 与 $H_1$ 均为对未知总体的提出的假设：
- 只能基于总体设定假设，比如 $$H_0: \mu = 12$$
- 不能根据样本设定假设，比如 $$H_0: \bar{x} \ge 12$$ 



### 显著性水平

使用 $H_0$ 表示我们要拒绝的假设：
- $H_0 = \text{true}$ 表明假设不成立（假设与现实不符）
- $H_0 = \text{false}$ 表明假设成立（假设与现实相符）


当我们进行决策时，可能存在 4 种情况：
- 拒绝 $H_0$ 且 $H_0 = \text{true}$（第一类错误）
- 拒绝 $H_0$ 且 $H_0 = \text{false}$ (正确)
- 未拒绝 $H_0$ 且 $H_0 = \text{true}$（正确）
- 未拒绝 $H_0$ 且 $H_0 = \text{false}$（第二类错误）


其中存在两种错误的情况：
- 第一类错误（Type-1 Error）：拒绝了正确的假设，也称为 **假阳性（False Positive）**
- 第二类错误（Type-2 Error）：未拒绝错误的假设，也称为 **假阴性（False Negative）**

由于抽样本身存在误差，这两类错误都是不可避免的，两者发生的概率分别为：
- 第一类错误 $$\alpha = P(\text{Reject } H_0|H_0)$$
- 第二类错误 $$\beta =  P(\text{Accept } H_0|H_1)$$

>  显著性水平 α 用于决定是否应该拒绝 $$H_0$$

通过设置显著性水平，可以控制这两类错误发生的概率，原理如下：
1. 假设 $H_0$ 基于总体均值 $\mu$ 进行设定
2. 显著性水平 $\alpha$ 越低，意味着置信区间 $1-\alpha$越大
3. 样本的均值 $\bar x$ 落在置信区间外的概率越低
4. 这意味着需要更强的证据才能拒绝 $H_0$


调整显著性水平会同时影响两类错误发生的概率：
- 更低的显著性水平 $\alpha$，会使得 $H_0$ 更容易被接受，更难以被拒绝。\
  因此可以降低第一类错误风险，但会增大第二类错误风险。
- 更高的显著性水平 $\alpha$，会使得 $H_0$ 更容易被拒绝，更难以被接受。\
  因此可以降低第二类错误风险，但会增大第一类错误风险。


假设检验中，另一个需要关注的指标是 **检验功效（Power of Test）**。\
其计算方式为 $1-β = P(\text{Reject } H_0|H_1)$，即：

> 当 $H_1$ 成立时，成功地拒绝 $H_0$ 的概率

功效越高，检验敏感度越高，越能发现真实存在的统计效应。


### 检验统计量

设计检验时，需要根据数据的类型、分布、样本大小以及要检验的假设类型，选择合适的统计检验方法，例如：

- Z检验：当总体标准差 $\sigma$ 已知且样本量很大时，判断样本均值 $\bar x$ 与 $H_0$ 中的总体均值 $\mu$ 是否一致
- t检验：当总体标准差 $\sigma$ 未知或样本量较少时，判断样本均值 $\bar x$ 与 $H_0$ 中的总体均值 $\mu$ 是否一致
- F检验：通过计算两组样本的方差，判断两者的总体方差是否相等
- 卡方检验：通过计算卡方统计量，判断两组样本是否服从相同的总体分布


不同的检验方法对应不同的 **检验统计量（Test Statistic）**

- 单样本 Z 检验的统计量为 $Z=\frac{\bar{x}-\mu}{\sigma/\sqrt n}$
  - $μ$ 为假设的总体均值
  - $σ$ 为已知的总体标准差
  - $\bar{x}$ 为样本均值
  - $n$ 为样本量

- 单样本 t 检验的统计量为 $T=\frac{\bar X-\mu}{s\sqrt n}$
  - $μ$ 为假设的总体均值
  - $\bar{x}$ 为样本均值
  - $s$ 为样本标准差
  - $n$ 为样本量

- 总体方差 F 检验的统计量为 $F=\frac{s_1^2}{s_2^2}$
  - $s_1^2$ 是较大的样本方差
  - $s_2^2$ 是较小的样本方差

- 卡方检验的统计量 $\chi^2=\sum\frac{(O_i-E_i)^2}{E_i}$
  - $E_i$ 为假设的**期望频数（Expected Frequency）**
  - $O_i$ 为样本的**观察频数（Observed Frequency）**


可以看到，统计量都是使用随机变量进行表示，这些随机变量都服相应的概率分布。

基于假设与样本计算出统计量后，接下来就是根据显著性水平 $\alpha$ 来判断是否拒绝 $H_0$。


### 检验类型

为了方便理解，后续将将基于 Z 均值检验进行介绍。

已知总体分布的方差为 $σ = 3$ ，基于假设 $μ = 66.7$，可以设计 3 类检验：

- **右尾检验（Right-Tailed Test）**
  - $H_0: μ ≤ 66.7$
  - $H_1: μ > 66.7$

- **左尾检验（Left-Tailed Test）**
  - $H_0: μ ≥ 66.7$
  - $H_1: μ < 66.7$

- **双尾检验（Two-Tailed Test）**
  - $H_0: μ = 66.7$
  - $H_1: μ ≠ 66.7$

选择哪种检验类型，取决于你的研究问题和你想在数据中寻找的差异方向：
|检验类型|$H_1$ 形式|拒绝域位置|检验目的|
|:-:|:-:|:-:|:-:|
|右尾检验|参数 > 某个值|右侧|证明参数“增加了”或“大于”某个基准值|
|左尾检验|参数 < 某个值|左侧|证明参数“减少了”或“小于”某个基准值|
|双尾检验|参数 ≠ 某个值|两侧|证明参数“不同于”某个基准值，不关心是增大还是减小|


### 得出结论


根据样本计算出检验统计量后，我们有两种方法来做决定：

- 使用检验统计量计算 p-value，比较 p-value 与显著性水平 $\alpha$
- 根据显著性水平 $\alpha$ 计算临界值，比较检验统计量与临界值


#### p-value

p-value 代表在 $H_0 = \text{true}$ 的情况下，观察到当前数据或更极端数据的概率。

p-value 是与预先设定的显著性水平 α 进行比较的：
- $\text{p-value} ≤α$ 意味着观测结果在 $H_0 = \text{true}$ 时非常罕见，这提供了强有力的证据来拒绝原假设
- $\text{p-value} >α$ 意味着观测结果在 $H_0 = \text{true}$ 时比较常见，这表明没有足够的证据来拒绝原假设


p-value 本质上是一种概率，如果将统计量看作随机变量的取值，则 p-value 可以通过 CDF 进行计算：
- Z统计量是随机变量 $X \sim Z$  的观测值，通过 $F_X(x)$ 可以计算统计量 $x$ 对应的 p-value
- 卡方统计量是随机变量 $X \sim \chi^2$ 的观测值，通过 $F_X(x$) 可以计算统计量 $x$ 对应的 p-value


使用 $X$ 表示假设的总体分布，$x_{\text{obs}}$ 表示观测到的样本统计量，不同检验类型的 p-value 计算方式为：

- 左尾检验 $\text{p-value}=P(X≤x_{\text{obs}}∣H_0=true)=F_X(x_{\text{obs}})$
- 右尾检验 $\text{p-value}=P(X≥x_{\text{obs}}∣H_0=true)=1-F_X(x_{\text{obs}})$
- 对称分布的双尾检验 $\text{p-value}=2×P(X≥∣x_{\text{obs}}∣∣H_0=true)=2×F_X(∣x_{\text{obs}}∣)$

回到之前 Z 均值检验的例子，基于 p-value 的决策流程如下：
- [右尾检测：$$H_1: \mu > 66.7$$](500x350:/tool/ztest-visualizer?alpha=5&tail=right&mu=66.7&sigma=3&n=10&sampleMean=68.442)
  - 样本观测值 $\bar x = 68.442$
  - 计算统计量 $Z = \frac{68.442 - 66.7}{3/\sqrt{10}} = 1.8362$
  - 计算 CDF $F_Z(1.8362) = 0.9668 $
  - 右尾 p-value $$P(\hat{X}>68.442|\mu=66.7) = 1 - 0.9668 =0.0332$$
  
  由于 p-value < 0.05，拒绝 $$H_0$$ 是合理的，$$H_1$$成立


- [双尾检验 $$H_1: \mu \ne 66.7$$](500x350:/tool/ztest-visualizer?alpha=5&tail=two&mu=66.7&sigma=3&n=10&sampleMean=68.442)
  - 样本观测值 $\bar x = 68.442$
  - 计算统计量 $Z = \frac{68.442 - 66.7}{3/\sqrt{10}} = 1.8362$
  - 计算 CDF $F_Z(1.8362) = 0.9668 $
  - 双尾 p-value 为 $$P(|\hat{X}-66.7|>|68.442 - 66.7|\ |\mu=66.7) = (1-0.9668) * 2 = 0.0663$$
  
  由于 p-value > 0.05，接受 $$H_0$$ 是合理的，$$H_1$$不成立

- [左尾检验 $$H_1: \mu < 64.252$$](500x350:/tool/ztest-visualizer?alpha=5&tail=left&mu=66.7&sigma=3&n=10&sampleMean=64.252)
  - 样本观测值 $\bar x = 64.252$
  - 计算统计量 $Z = \frac{64.252 - 66.7}{3/\sqrt{10}} = −2.581$
  - 计算 CDF $F_Z(−2.581) = 0.0049 $
  - 左尾 p-value 为 $$P(\hat{X}<64.252|\mu=66.7) = 0.0049 $$

  此时 p-value < 0.05，拒绝 $$H_0$$ 是合理的，$$H_1$$成立


#### 临界值

另一种更直观的形式是比较 **临界值（Critical Value）**。\
临界值与统计量具有相同量纲，两者可以直接进行比较，因此更为直观。\
为了将显著性水平转换为对应的临界值，需要借助 Inverse CDF。

使用 α 表示显著性水平，不同检验类型的临界值计算方式为

- 左尾检验 $k_{\alpha}=F^{−1}(α)$
- 右尾检验 $k_{\alpha}=F^{−1}(1-α)$
- 对称分布的双尾检验 $k_{\alpha 2}=F_X^{−1}(α/2),\ k_{\alpha 1}=F_X^{−1}(1-\frac{α}2)$

只要观测值 $x_{\text{obs}}$（这里不是统计量）比临界值 $k_{\alpha}$ 更极端，则我们有足够的理由拒绝 $H_0$：
- 左尾检验 $x_{\text{obs}} < k_{\alpha}$
- 右尾检验 $x_{\text{obs}} > k_{\alpha}$
- 对称分布的双尾检验 $x_{\text{obs}} < k_{\alpha 2} \text{ or } x_{\text{obs}} > k_{\alpha 1}$

回到之前 Z 均值检验的例子，基于 p-value 的决策流程如下：
- [右尾检测：$$H_1: \mu > 66.7$$](500x350:/tool/ztest-visualizer?alpha=5&tail=right&mu=66.7&sigma=3&n=10&sampleMean=68.442)
  - 样本观测值 $\bar x = 68.442$
  - 计算逆 CDF $F_Z^{-1}(1-0.05) = 1.645 $
  - 将 Z 分布映射至 $\mathcal N(66.7,3)$ 得到右尾临界值 $k_{\alpha} = 66.7 + 1.645 \times (3/\sqrt{10}) = 68.2607$

  由于 $68.442 > 68.2607$，拒绝 $$H_0$$ 是合理的，$$H_1$$成立

- [双尾检验 $$H_1: \mu \ne 66.7$$](500x350:/tool/ztest-visualizer?alpha=5&tail=two&mu=66.7&sigma=3&n=10&sampleMean=68.442)
  - 样本观测值 $\bar x = 68.442$
  - 计算逆 CDF 
    - $F_{Z 2}^{-1}(0.05/2) = −1.96$
    - $F_{Z 1}^-1(1-0.05/2) = 1.96$
  - 将 Z 分布映射至 $\mathcal N(66.7,3)$ 得到临界值 
    - $k_{\alpha 2} = 66.7 - |−1.96| \times (3/\sqrt{10}) = 64.8406$
    - $k_{\alpha 1} = 66.7 + |1.96| \times (3/\sqrt{10}) = 68.5594$
  
  由于 $68.442 \in [64.8406,68.5594]$，接受 $$H_0$$ 是合理的，$$H_1$$不成立

- [左尾检验 $$H_1: \mu < 64.252$$](500x350:/tool/ztest-visualizer?alpha=5&tail=left&mu=66.7&sigma=3&n=10&sampleMean=64.252)
  - 样本观测值 $\bar x = 64.252$
  - 计算逆 CDF $F_Z^{-1}(0.05) = -1.645 $
  - 将 Z 分布映射至 $\mathcal N(66.7,3)$ 得到左尾临界值 $k_{\alpha} = 66.7 - 1.645 \times (3/\sqrt{10}) = 65.1393$
  
  由于 $64.252 < 65.1393$，拒绝 $$H_0$$ 是合理的，$$H_1$$成立


### 流程总结

最后总结一下假设检验的基本步骤：
1. 提出互斥假设
2. 选择显著性水平
3. 选择合适的统计检验方法
4. 计算检验统计量
5. 根据对应的 p-value 或临界值得出结论

注意：不拒绝原假设并不意味着原假设是真，只是说没有足够证据证明它是假。